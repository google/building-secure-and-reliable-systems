<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Building Secure and Reliable Systems</title>
  <link rel="stylesheet" type="text/css" href="theme/html/html.css">
</head>
<body data-type="book">
<section xmlns="http://www.w3.org/1999/xhtml" data-type="chapter" id="twozero_understanding_roles_and_respons">
<h1>Understanding Roles and Responsibilities</h1>

<p class="byline">By Heather Adkins, Cyrus Vesuna, Hunter King, <span class="keep-together">Felix Gröbert, and David Challoner</span></p>
<p class="byline cont">with Susanne Landers, Steven Roddis, Sergey Simakov, <span class="keep-together">Shylaja Nukala,</span> Janet Vong, Douglas Colish, Betsy Beyer, <span class="keep-together">and Paul Blankinship</span></p>

<aside data-type="sidebar" id="this_chapter_addresses_the_question_of">
<p><a contenteditable="false" data-primary="roles and responsibilities" data-type="indexterm" id="ch20.html0">&nbsp;</a>This chapter addresses the question of who should work on security. We challenge the common myth that security is a topic that only experts should handle. Instead, we argue that <em>everyone</em> is responsible for security, though you may need a security specialist in some instances. We also address the role of security experts in a world where security is tightly integrated into the lifecycle of systems, and therefore handled by other types of professionals. Finally, we conclude with a look at some of the specialist options available to support an organization, especially as it grows over time.</p>
</aside>
<p>As this book emphasizes many times, building systems is a <em>process</em>, and the processes for improving security and reliability rely on people. This means that building secure and reliable systems involves tackling two important questions:</p>
<ul>
<li><p>Who is responsible for security and reliability in the organization?</p></li>
<li><p>How are security and reliability efforts integrated into the organization?</p></li>
</ul>
<p>The answer to these questions is highly dependent on your organization’s objectives and culture (the topic of the next chapter). The following sections lay out some high-level guidance for how to think about these questions, and offer insight into how Google has approached them over time.</p>
<aside data-type="sidebar" id="on_reliability">
<h5>On Reliability</h5>
<p>Reliability-related roles and responsibilities are covered in other resources, so the majority of this chapter provides comparable insights on the security side. For more on the various ways in which SREs might engage with the reliability needs of an organization, and how that model may evolve over time, see <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/evolving-sre-engagement-model/">Chapter 32 of the SRE book</a> and Chapters <a class="orm:hideurl" href="https://landing.google.com/sre/workbook/chapters/engagement-model/">18</a>, <a class="orm:hideurl" href="https://landing.google.com/sre/workbook/chapters/reaching-beyond/">19</a>, and <a class="orm:hideurl" href="https://landing.google.com/sre/workbook/chapters/team-lifecycles/">20</a> of the SRE workbook.</p>
</aside>
<section data-type="sect1" id="who_is_responsible_for_security_and_rel">
<h1>Who Is Responsible for Security and Reliability?</h1>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="security/reliability as everyone&#39;s responsibility" data-type="indexterm" id="ch20.html1">&nbsp;</a>Who works on security and reliability in a given organization? We believe that security and reliability should be integrated into the lifecycle of systems; therefore, they’re everyone’s responsibility. We’d like to challenge the myth that organizations should place the burden for these concerns solely on dedicated experts.</p>

<p>If reliability and security are delegated to an isolated team of people who can’t mandate that other teams make security-related changes, the same failures will happen repeatedly. Their task may start to feel Sisyphean—repetitive and unproductive.</p> 

<p>We encourage organizations to make reliability and security the responsibility of <em>everyone</em>: developers, SREs, security engineers, test engineers, tech leads, managers, project managers, tech writers, executives, and so on. That way, the nonfunctional requirements described in <a data-type="xref" href="#design_tradeoffs">Chapter 4</a> become a focus for the whole organization throughout a system’s entire lifecycle.</p>
<aside data-type="sidebar" id="a_system_security_and_reliability_analo">
<h5>A System Security and Reliability Analogy</h5>
<p>Modern cars offer a good analogy for the way security and reliability are embedded into system design and delivery. Almost every component of a car incorporates both security and reliability in some way. Seats are designed to handle a crash, a windshield needs to crack safely, and headlights are angled to avoid blinding oncoming traffic. Seat belts must withstand being latched thousands of times. The windshield has to repel all types of weather, and the headlights must always turn on when you need them. A car’s digital systems must be similarly hardened. Everyone who plays a part in building the car must do this work—not just safety and reliability experts.</p>
</aside>
<section data-type="sect2" id="the_roles_of_specialists">
<h2>The Roles of Specialists</h2>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="specialists&#39; roles" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="security specialists" data-secondary="role of" data-type="indexterm">&nbsp;</a>If everyone is responsible for security and reliability, then you might wonder: what exactly is the role of a security specialist or reliability expert? According to one school of thought, the engineers building a given system should primarily focus on its core functionality. For example, developers might focus on building a set of critical user journeys for a mobile phone–based app. Complementing the work of the developer team, a security-focused engineer will look at the app from the perspective of an attacker aiming to undermine its safety. A reliability-focused engineer can help understand the dependency chains and, based on these, identify what metrics should be measured that will lead to happy customers and an SLA-compliant system. This division of labor is common in many development environments, but it’s important that these types of roles work together rather than in isolation.</p>
<p>To expand on this idea further, depending on the complexity of a system, an organization may need people with specialized experience to make nuanced judgment calls. Since it’s not possible to build an absolutely secure system that’s resilient against every attack, or a system that is perfectly reliable, advice from experts can help steer development teams. Ideally, this guidance should be integrated into the development lifecycle. This integration can take multiple forms, and security and reliability specialists should work directly with developers or other specialists that consult at each stage of the lifecycle to improve systems.<sup><a data-type="noteref" id="ch20fn1-marker" href="#ch20fn1">1</a></sup> For example, security consultation can happen in multiple stages:</p>
<ul>
<li><p>A <em>security design review</em> at the outset of a project to determine how security is integrated</p></li>
<li><p><em>Ongoing security audits</em> to make sure a product is built correctly per security specifications</p></li>
<li><p><em>Testing</em> to see what vulnerabilities an independent person can find</p></li>
</ul>
<aside data-type="sidebar" id="security_and_reliability_risk_evaluatio">
<h5>Security and Reliability Risk Evaluation</h5>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="security/reliability risk evaluation" data-type="indexterm">&nbsp;</a>Nuanced security and reliability advice can be helpful in making judgment calls about risk. For example, suppose developers working on a project haven’t had time to build in a desired security protection or think about graceful degradation, but need to launch the product or system soon. <a contenteditable="false" data-primary="Site Reliability Engineer/Engineering (SRE)" data-secondary="risk evaluation by" data-type="indexterm">&nbsp;</a>A security engineer and an SRE can help the organization understand what might happen if it’s launched in its current state. Will adversaries be able to attack a vulnerable system? Will the global system go down if user traffic for a certain country is higher than expected? What is the likelihood that either of these events will happen? Is the organization equipped with temporary mitigations for potential issues that might arise? Experienced security engineers and SREs can offer valuable advice in these situations.</p>
</aside>
<p>Security experts should be responsible for implementing security-specific technologies that require specialist knowledge. <a contenteditable="false" data-primary="cryptography" data-type="indexterm">&nbsp;</a>Cryptography is the canonical example: “don’t roll your own crypto” is a common industry catchphrase meant to discourage enterprising developers from implementing their own solutions. Cryptography implementations, whether in libraries or hardware, should be left to experts. If your organization needs to provide secure services (such as a web service over HTTPS), use industry-accepted and verified solutions instead of attempting to write your own encryption algorithm. Specialist security knowledge can also be required to implement other types of highly complex security infrastructure, such as custom authentication, authorization, and auditing (AAA) systems, or new secure frameworks to prevent common security vulnerabilities.</p>
<p>Reliability engineers (such as SREs) are best positioned to develop centralized infrastructure and organization-wide automation. <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/automation-at-google/">Chapter 7 of the SRE book</a> discusses the value and evolution of horizontal solutions, and shows how critical software that enables product development and launches can evolve into a platform.</p>
<p>Finally, specialists in security and reliability can devise best practices, policies, and training tailored to your organization’s workflows. These tools should empower developers to adopt best practices and implement effective security and reliability practices. A specialist should aim to build a brain trust of knowledge for the organization by constantly educating themselves on developments in the industry and generating broader awareness (see <a data-type="xref" href='ch21.html#culture_of_awareness'>Culture of Awareness</a>). In creating awareness, a specialist can help the organization become more secure and reliable in an iterative way. For example, Google has SRE- and security-focused educational programs that provide a baseline level of knowledge to all new hires in these specific roles. In addition to making the course material available company-wide, we also offer employees many self-study courses on these topics.</p>
</section>
<section data-type="sect2" id="understanding_security_expertise">
<h2>Understanding Security Expertise</h2>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="understanding security expertise" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="security specialists" data-secondary="hiring" data-type="indexterm">&nbsp;</a>Anyone who has tried to hire security professionals into their organization knows that the task can be challenging. If you’re not a security specialist yourself, what should you look for when hiring one? Medical professionals provide a good analogy: most have a general understanding of the fundamentals of human health, but many specialize at some point. In the medical field, family doctors or general practitioners are typically responsible for primary care, but more serious conditions may call for a specialist in neurology, cardiology, or some other area. Similarly, all security professionals tend to command a general body of knowledge, but they also tend to specialize in a few specific areas.</p>
<p>Before you hire a security specialist, it’s important to know the types of skills your organization will need. If your organization is small—for example, if you’re a startup or an open source project—a generalist may cover many of your needs. As your <span class="keep-together">organization</span> grows and matures, its security challenges may become more complex and require increased specialization. <a data-type="xref" href="#security_expertise_needed_at_key_milest">#security_expertise_needed_at_key_milest</a> presents some key milestones in Google’s early history that needed corresponding security expertise.</p>
<table class="border" id="security_expertise_needed_at_key_milest">
<caption>Security expertise needed at key milestones in Google’s history</caption>
<thead>
<tr>
<th width="20%">Company milestones</th>
<th width="30%">Expertise needed</th>
<th>Security challenges</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google Search (1998)<br/><em>Google Search provides users with the ability to find publicly available information.</em></td>
<td>General</td>
<td>Search query log data protection<br/>Denial-of-service protection<br/>Network and system security</td>
</tr>
<tr>
<td>Google AdWords (2000)<br/><em>Google AdWords (Google Ads) enables advertisers to show ads on Google Search and other products.</em></td>
<td>General<br/>Data security<br/>Network security<br/>Systems security<br/>Application security<br/>Compliance and audit<br/>Anti-fraud<br/>Privacy<br/>Denial of service<br/>Insider risk</td>
<td>Financial data protection<br/>Regulatory compliance<br/>Complex web applications<br/>Identity<br/>Account abuse<br/>Fraud and insider abuse</td>
</tr>
<tr>
<td>Blogger (2003)<br/><em>Blogger is a platform that allows users to host their own web pages.</em></td>
<td>General<br/>Data security<br/>Network security<br/>Systems security<br/>Application security<br/>Content abuse<br/>Denial of service</td>
<td>Denial of service<br/>Platform abuse<br/>Complex web applications</td>
</tr>
<tr>
<td>Google Mail (Gmail) (2004)<br/><em>Gmail is Google’s free webmail system, with advanced features available via a paid GSuite account.</em></td>
<td>General<br/>Privacy<br/>Data security<br/>Network security<br/>Systems security<br/>Application security<br/>Cryptography<br/>Anti-spam<br/>Anti-abuse<br/>Incident response<br/>Insider risk<br/>Enterprise security</td>
<td>Protecting highly sensitive user content at rest and in transit<br/>Threat models involving highly capable external attackers<br/>Complex web applications<br/>Identity systems<br/>Account abuse<br/>Email spam and abuse<br/>Denial of service<br/>Insider abuse<br/>Enterprise needs</td>
</tr>
</tbody>
</table>
</section>
<section data-type="sect2" id="certifications_and_academia">
<h2>Certifications and Academia</h2>
<p><a contenteditable="false" data-primary="certification (security specialists)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="security specialists" data-secondary="certifications and academia" data-type="indexterm">&nbsp;</a>Some security experts seek to earn certifications in their field of interest. Security-focused industry certifications are offered by institutions worldwide, and can be good indicators of someone’s interest in developing relevant skills for their career and their ability to learn key concepts. These certifications typically involve a standardized knowledge-based test. Some certifications require a minimum amount of classroom, conference, or job experience. Nearly all expire after a certain amount of time, or require certificants to refresh minimum requirements.</p>
<p>These standardized testing mechanisms may not necessarily attest to someone’s aptitude for success in a security role at your organization, so we recommend taking a balanced approach to assessing security specialists, considering all of their qualifications in totality: their practical experience, certifications, and personal interest. While certifications may speak to someone’s ability to pass exams, we have seen credentialed professionals who’ve had difficulty applying their knowledge to solving problems. At the same time, early career candidates, or those coming to the field from other specialist roles, may use certifications to upgrade their knowledge quickly. With a keen interest in the field, or practical experience with open source projects (instead of workplace experience), such early career candidates may be able to add value quickly.</p>
<p>Because security experts are increasingly in demand, many industries and universities have been developing and evolving security-focused academic programs. Some institutions offer general security-focused degrees that cover many security domains. Other degree programs concentrate on a specific security domain (which is common for doctoral students), and some offer a blended curriculum that focuses on the overlap between cybersecurity issues and domains such as public policy, law, and privacy. As with certifications, we recommend considering a candidate’s academic achievements in the context of their practical experience and your organization’s needs.</p>
<p>For example, you might want to bring on an experienced professional as your first security hire, and then hire early career talent once the team is established and can offer mentorship. Alternatively, if your organization is working on a niche technical problem (such as securing self-driving cars), a new PhD graduate with deep knowledge in that specific research area but little work experience might fit the role nicely.<a contenteditable="false" data-primary="" data-startref="ch20.html1" data-type="indexterm">&nbsp;</a></p>
</section>
</section>
<section data-type="sect1" id="integrating_security_into_the_organizat">
<h1>Integrating Security into the Organization</h1>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="integrating security into the organization" data-type="indexterm" id="ch20.html2">&nbsp;</a><a contenteditable="false" data-primary="security (generally)" data-secondary="integrating into the organization" data-type="indexterm" id="ch20.html3">&nbsp;</a>Knowing when to start working on security is more of an art than a science. Opinions on this topic are plentiful and varied. However, it’s generally safe to say that the sooner you start thinking about security, the better off you’ll be. In more concrete terms, we’ve observed certain conditions over the years that are likely to prompt organizations (including our own) to start building a security program:</p>
<ul>
<li><p>When an organization begins to handle data of a personal nature, such as logs of sensitive user activity, financial information, health records, or email</p></li>
<li><p>When an organization needs to build highly secure environments or custom technologies, such as custom security features in a web browser</p></li>
<li><p>When regulations require adherence to a standard (such as Sarbanes-Oxley, PCI DSS, or GDPR) or a related audit<sup><a data-type="noteref" id="ch20fn2-marker" href="#ch20fn2">2</a></sup></p></li>
<li><p>When an organization has contractual requirements with customers, especially around breach notification or minimum security standards</p></li>
<li><p>During or after a compromise or data breach (ideally, before)</p></li>
<li><p>As a reaction to the compromise of a peer operating in the same industry</p></li>
</ul>
<p>In general, you’ll want to start working on security far before any of these conditions are met, and especially before a data breach! It’s far simpler to implement security before, rather than after, such an event. For example, if your company plans to launch a new product that accepts online payments, you may want to consider a specialty vendor for that functionality. Vetting a vendor and ensuring that they have good data handling practices will take time.</p>
<p>Imagine that you launch with a vendor that doesn’t integrate the online payment system securely. A data breach could incur regulatory fines, loss of customer trust, and a hit to productivity as your engineers reimplement the system correctly. Many organizations cease to exist after such incidents.<sup><a data-type="noteref" id="ch20fn3-marker" href="#ch20fn3">3</a></sup></p>
<p>Similarly, imagine that your company is signing a new contract with a partner that has additional data handling requirements. Hypothetically, your legal team may advise you to implement those requirements before signing the contract. What might happen if you delay those extra protections and suffer a breach as a result?</p>
<p>Related questions often arise when considering the cost of a security program and the resources your company can invest in the program: how expensive is implementing security, and can the company afford it? While this chapter can’t cover this very complex topic deeply, we’ll emphasize two main takeaways.</p>
<p>First off, for security to be effective, it must be carefully balanced with your organization’s other requirements. To put this guideline in perspective, we can make Google nearly 100% safe from malicious actors by turning off our datacenters, networks, computing devices, and so on. While doing so would achieve a high level of safety, Google would no longer have customers and would disappear into the annals of failed companies. Availability is a core tenet of security! In order to craft a reasonable security strategy, you need to understand what the business requires to operate (and in the case of most companies, what it takes to earn a profit). Find the right balance between the requirements of your business and adequate security controls.</p>
<p>Secondly, security is everyone’s responsibility. You can reduce the cost of some security processes by distributing it among the teams affected the most. For example, consider a company that has six products, each staffed with a product team and protected by 20 firewalls. In this scenario, one common approach is to have a central security team maintain the configuration of all 120 firewalls. This setup requires the security team to have extensive knowledge of six different products—a recipe for eventual reliability issues or delays in system changes, all of which can increase the cost of your security program. An alternative approach is to assign responsibility to the security team for operating an automated configuration system that accepts, validates, approves, and pushes firewall changes proposed by the six product teams. This way, each product team can efficiently propose minor changes for review and scale the configuration process. These kinds of optimizations can save time and even improve system reliability by catching errors early without human involvement.</p>
<p>Because security is such an integral part of an organization’s lifecycle, nontechnical areas of the organization also need to consider security early on. For example, boards of directors often examine the security and privacy practices of the entities they oversee. Lawsuits in the aftermath of data breaches, such as the shareholder suit against Yahoo! in 2017, are driving this trend.<sup><a data-type="noteref" id="ch20fn4-marker" href="#ch20fn4">4</a></sup> When preparing your roadmap for security, be sure to consider these types of stakeholders in your process.</p>
<p>Finally, it’s important to create processes for maintaining a constant understanding of the current issues you need to address, along with their priorities. When treated as a continuous process, security requires an ongoing assessment of the risks the business is facing. In order to iterate defense-in-depth security controls over time, you need to incorporate risk assessment into your software development lifecycle and security practices. The next section discusses some practical strategies for doing so.</p>
<section data-type="sect2" id="embedding_security_specialists_and_secu">
<h2>Embedding Security Specialists and Security Teams</h2>
<p><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="embedding security specialists and security teams" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="security specialists" data-secondary="embedding into the organization" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="security teams" data-seealso="Blue Teams; Chrome security team; Red Teams" data-type="indexterm">&nbsp;</a>Over the years, we’ve seen many companies experiment with where to embed security specialists and security teams inside their organizations. The configurations have ranged from fully embedded security specialists inside product teams (see <a data-type="xref" href="#onenine_case_study_chrome_security_team">Chapter 19</a>) to fully centralized security teams. Google’s central security team is organizationally configured as a hybrid of both options.</p>
<p>Many companies also have different accountability arrangements for decision making. We’ve seen Chief Information Security Officers (CISOs) and other leadership roles responsible for security report to just about every C-level executive: the CEO, CFO, CIO, or COO, the general counsel for the company, a VP of Engineering, and even the CSO (Chief Security Officer, usually responsible also for physical security). There is no right or wrong configuration, and the choice your organization makes will be highly dependent on what’s most effective for your security efforts.</p>
<p>The rest of this section offers some details on configuration options that we’ve had success with over the years. While we’re a big technology company, many of these components also work well in small or medium-sized organizations. However, we imagine this configuration may not work well for a financial company or a public utility, where accountability for security may have different stakeholders and drivers. Your mileage may vary.</p>
</section>
<section data-type="sect2" id="example_embedding_security_at_google">
<h2>Example: Embedding Security at Google</h2>
<p><a contenteditable="false" data-primary="Google" data-secondary="embedding security at" data-type="indexterm" id="ch20.html4">&nbsp;</a><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="embedding security at Google" data-type="indexterm" id="ch20.html5">&nbsp;</a>At Google, we first built out a central security organization that operates as a peer to product engineering. The head of this organization is a senior leader within engineering (a VP). This creates a reporting structure in which security is seen as an engineering ally, but also allows the security team sufficient independence to raise issues and resolve disputes without conflicts of interest other leaders may have. This is similar to the way the SRE organization at Google maintains separate reporting chains from product development teams.<sup><a data-type="noteref" id="ch20fn5-marker" href="#ch20fn5">5</a></sup> In this way, we create an open and transparent engagement model that focuses on improvements. Otherwise, you risk having a team with the following characteristics:</p>
<ul>
<li><p>Unable to raise serious issues because launches are overprioritized</p></li>
<li><p>Seen as a blocking gate that needs to be circumvented organizationally via silent launches</p></li>
<li><p>Slowed down by insufficient documentation or code access</p></li>
</ul>
<p>Google’s central security team relies on standard processes like ticketing systems to interact with the rest of the organization when teams need to request a design review, an access control change, and so on. For a sense of how this workflow functions, see <a data-type="xref" href='#googleapostrophes_smart_system_for_inta'>#googleapostrophes_smart_system_for_inta</a>.</p>
<p><a contenteditable="false" data-primary="security champions" data-type="indexterm">&nbsp;</a>As Google has grown, it has also become useful to embed a “security champion” within individual product engineering peer groups. The security champion becomes the gateway to facilitate collaboration between the central security team and the product team. When starting out, this role is ideal for senior engineers with good standing in the organization and an interest or a background in security. These engineers also become the technical leads for product security initiatives. As product teams become more complex, this role is assigned to a senior decider, such as a director or VP—this person can make tough calls (such as balancing launches versus security fixes), acquire resources, and resolve conflicts.</p>
<p>In the security champion model, it’s important to establish and agree upon an engagement process and responsibilities. For example, the central team may continue to perform design reviews and audits, set organization-wide security policies and standards, build safe application frameworks (see <a data-type="xref" href="#onethree_testing_code">Chapter 13</a>), and devise common infrastructure such as least privilege methods (see <a data-type="xref" href="#design_for_least_privilege">Chapter 5</a>). Distributed security champions are key stakeholders for these activities, and should help decide how these controls will work in their product teams. The security champions also drive the implementation of policies, frameworks, infrastructure, and methods within their respective product teams. This organizational configuration requires a tight communication loop through team charters, cross-functional meetings, mailing lists, chat channels, and so on.</p>
<p>Because of Google and Alphabet’s large size, in addition to a central security team and distributed security champions, we also have special decentralized security teams for more complex products. <a contenteditable="false" data-primary="Android security team" data-type="indexterm">&nbsp;</a>For example, the Android security team sits within the Android engineering organization. <a contenteditable="false" data-primary="Chrome security team" data-type="indexterm">&nbsp;</a>Chrome has a similar model (see <a data-type="xref" href="#onenine_case_study_chrome_security_team">Chapter 19</a>). This means the Android and Chrome security teams are responsible for the end-to-end security of their respective products, which includes deciding on product-specific standards, frameworks, infrastructure, and methods. These specialized security teams run the product security review process and have special programs to harden the products. For example, the Android security team has worked to <a href="https://security.googleblog.com/2019/05/queue-hardening-enhancements.html">harden the media stack</a> and has benefited from an integrated security and engineering approach.</p>
<p>In all of these models, it’s important for the security team to be open and approachable. In addition to a security review process, during which developers can receive help from subject matter experts, engineers need timely and consistent feedback on security-related issues throughout the project lifecycle. We address a number of cultural issues around these interactions in <a data-type="xref" href="#twoone_building_a_culture_of_security_a">Chapter 21</a>.</p>
<aside data-type="sidebar" id="googleapostrophes_smart_system_for_inta">
<h5>Google’s Smart System for Intake</h5>
<p><a contenteditable="false" data-primary="Google" data-secondary="smart system for intake" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="intake" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="ticket queue" data-type="indexterm">&nbsp;</a>In many organizations, a ticket queue is the only communication channel to the security team. The “one size fits most” nature of tickets typically results in a lot of back and forth in order to extract relevant information. To help our teams save time, we built a smart system as our ticket queue frontend to automate away as much of the consulting process as possible.</p>
<p>Instead of providing a simple form or forms as input into our queue, we built a system with a dynamic questionnaire. As the user describes their request, the system automatically asks common security-related questions, and provides warnings and recommendations to educate them about risky decisions. These guiding questions help the user determine whether they are using a memory-safe language, applying a vetted templating system/framework, handling sensitive data, or modifying a critical system. After the user fills out the form, the system is able to identify an explicit problem and create a ticket to automatically route to the correct user or team. Then a security engineer can quickly parse the inherently structured information and relevant data, and help the user.</p>
<p>Since security work constantly evolves and the questionnaire won’t cover all use cases, the intake form allows users to bypass sections and choose an “other” option if their request doesn’t directly map to a defined workflow. To prevent users from defaulting to the “other” option, we explicitly say that this option is for one-off requests that are time-sensitive.</p>
<p>One key feature of the expert system is its ability to evolve and grow with the organization. If a large number of users skip our questionnaire, we know our expert system needs tweaking. Security engineers periodically examine the sections users most often bypass, and either add new question paths or modify overly burdensome question segments. The goal is to encourage users to focus on the main security questions they need to think about.</p>
<p>Building this system helped us accomplish the following:</p>
<ul>
<li><p>Let the user self-serve, and self-educate themselves in the process.</p></li>
<li><p>Save time for meaningful and productive collaboration in providing <span class="keep-together">a recommendation</span>.</p></li>
<li><p>Dramatically improve the overall speed of understanding the problem and providing a recommendation.</p></li>
<li><p>Greatly increase the overall number of tickets successfully closed per week.</p></li>
<li><p>Improve the quality of the information in tickets.<a contenteditable="false" data-primary="" data-startref="ch20.html5" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html4" data-type="indexterm">&nbsp;</a></p></li>
</ul>
</aside>
</section>
<section data-type="sect2" id="special_teams_blue_and_red_teams">
<h2>Special Teams: Blue and Red Teams</h2>
<p><a contenteditable="false" data-primary="Blue Teams" data-type="indexterm" id="ch20.html6">&nbsp;</a><a contenteditable="false" data-primary="Red Teams" data-type="indexterm" id="ch20.html7">&nbsp;</a><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="Blue and Red Teams" data-type="indexterm" id="ch20.html8">&nbsp;</a>Security teams are often tagged using colors to denote their role in securing an organization.<sup><a data-type="noteref" id="ch20fn6-marker" href="#ch20fn6">6</a></sup> All of these color-coded teams work toward the common goal of improving the security posture of the company.</p>
<p><a contenteditable="false" data-primary="Blue Teams" data-type="indexterm">&nbsp;</a><em>Blue Teams</em> are primarily responsible for assessing and hardening software and infrastructure. They’re also responsible for detection, containment, and recovery in the event of a successful attack. Blue Team members can be anyone in an organization who works on defending it, including the people who build secure and reliable <span class="keep-together">systems</span>.</p>
<p><a contenteditable="false" data-primary="Red Teams" data-secondary="defined" data-type="indexterm">&nbsp;</a><em>Red Teams</em> run offensive security exercises: end-to-end attacks that simulate realistic adversaries. These exercises reveal weaknesses in an organization’s defenses and test its ability to detect and defend against attacks.</p>
<p>Typically, Red Teams focus on the following:</p>
<dl>
<dt>A specific goal</dt>
<dd>For example, a Red Team might seek to exploit customer account data (or more specifically, to find and exfiltrate to a safe destination some customer account data that is available in your environment). Such exercises are very similar to the way adversaries operate.</dd>
<dt>Surveillance</dt>
<dd>The aim is to determine whether your detection methods can detect reconnaissance by an adversary. Surveillance can also serve as a map for future goal-based engagements.</dd>
<dt>Targeted attacks</dt>
<dd>The aim is to demonstrate the feasibility of exploiting security issues that are supposedly theoretical and very unlikely to be exploited. As a result, you can determine which issues merit building a defense.</dd>
</dl>
<p>Before starting a Red Team program, be sure to obtain buy-in from parts of the organization that might be affected by these exercises, including legal and executives. This is also a good time to define boundaries—for example, Red Teams should not access customer data or disrupt production services, and they should use approximations for data theft and service outages (e.g., by compromising only the data of test accounts). These boundaries need to strike a balance between conducting a realistic exercise and establishing a timing and scope that your partner teams are comfortable with. Of course, your adversaries won’t respect these boundaries, so Red Teams should pay extra attention to key areas that are not well protected.</p>
<p>Some Red Teams share their attack plans with the Blue Team, and work very closely with them to get fast and comprehensive insight into the detection situation. <a contenteditable="false" data-primary="Purple Team" data-type="indexterm">&nbsp;</a>This relationship can even be formalized with a Purple Team that bridges the two.<sup><a data-type="noteref" id="ch20fn7-marker" href="#ch20fn7">7</a></sup> This can be useful if you are conducting many exercises and want to move fast, or if you want to distribute Red Team activity among product engineers. This configuration can also inspire the Red Team to look in places it might not otherwise consider. The engineers that design, implement, and maintain systems know the system best, and usually have an instinct for where the weaknesses are.</p>
<aside data-type="sidebar" id="detecting_red_teams">
<h5>Detecting Red Teams</h5>
<p>If your Red and Blue Teams choose not to share information with each other, be sure to establish a protocol for what to do when the Blue Team detects the Red Team. Imagine this scenario: a Red Team successfully breaches your customer database. The Blue Team detects them and executes emergency response procedures, resulting in notifications to your executives, legal team, and regulators! A good protocol for deescalation after detection will prevent this sort of confusion.</p>
</aside>
<p>Red Teams are not vulnerability scanning or penetration testing teams. <a contenteditable="false" data-primary="vulnerability scanning teams" data-type="indexterm">&nbsp;</a><em>Vulnerability scanning teams</em> look for predictable and known weaknesses in software and configurations that can be automatically scanned for. <a contenteditable="false" data-primary="penetration testers" data-type="indexterm">&nbsp;</a><em>Penetration testing teams</em> focus more on finding a large set of vulnerabilities and the testers trying to exploit them. Their scope is narrower, focused on a particular product, infrastructure component, or process. As these teams mostly test prevention aspects and some detection aspects of an organization’s security defense, their typical engagement lasts days.</p>
<p>In contrast, Red Team engagements are goal-oriented and typically last weeks. Their goals are specific targets, such as intellectual property or customer data exfiltration. They are broadly scoped and use any means necessary to attain their goals (within safety limits) by traversing product, infrastructure, and internal/external boundaries.</p>
<p>Given time, good Red Teams can attain their goals, often without being detected. Rather than viewing a successful Red Team attack as a judgment of a poor or ineffective business unit, use this information to better understand some of your more complex systems in a blameless way.<sup><a data-type="noteref" id="ch20fn8-marker" href="#ch20fn8">8</a></sup> Use Red Team exercises as an opportunity to better learn how these systems are interconnected and how they share trust boundaries. Red Teams are designed to help bolster threat models and build defenses.</p>
<div data-type="note">
<p>Because they don’t exactly mirror the behavior of external attackers, Red Team attacks aren’t a perfect test of your detection and response capabilities. This is especially true if the Red Team is staffed by internal engineers who have existing knowledge about the systems they’re attempting to penetrate.</p>
<p>You also can’t feasibly conduct Red Team attacks frequently enough to provide a real-time view of your vulnerability to attacks or statistically significant metrics for your detection and response teams. Red Teams are meant to find the rare edge cases that normal testing cannot. All caveats aside, regularly conducting Red Team exercises is a good way to understand your security posture end to end.</p>
</div>
<p>You can also leverage Red Teams to teach the people who design, implement, and maintain systems about the adversarial mindset. Embedding these people directly into the attack team—for example, via a small-scoped project—will give them firsthand insight into how attackers scrutinize a system for possible vulnerabilities and work around defenses. They can inject this knowledge into their team’s development process later on.</p>
<p>Engaging with a Red Team helps you better understand your organization’s security posture and develop a roadmap for implementing meaningful risk reduction projects. By understanding the implications of your current risk tolerance, you can determine whether you need to make adjustments.<a contenteditable="false" data-primary="" data-startref="ch20.html8" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html7" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html6" data-type="indexterm">&nbsp;</a></p>
</section>
<section data-type="sect2" id="external_researchers">
<h2>External Researchers</h2>
<p><a contenteditable="false" data-primary="external researchers" data-type="indexterm" id="ch20.html9">&nbsp;</a><a contenteditable="false" data-primary="researchers" data-type="indexterm" id="ch20.html10">&nbsp;</a><a contenteditable="false" data-primary="roles and responsibilities" data-secondary="external researchers" data-type="indexterm" id="ch20.html11">&nbsp;</a>Another way to examine and improve your security posture is to work closely with outside researchers and enthusiasts who find vulnerabilities in your systems. As we mentioned in <a data-type="xref" href="#understanding_adversaries">Chapter 2</a>, this can be a useful way to get feedback about your systems.</p>
<p><a contenteditable="false" data-primary="bug bounties (Vulnerability Reward Programs)" data-type="indexterm" id="ch20.html12">&nbsp;</a><a contenteditable="false" data-primary="Vulnerability Reward Programs (bug bounties)" data-type="indexterm" id="ch20.html13">&nbsp;</a>Many companies work with outside researchers by establishing <em>Vulnerability Reward Programs (VRPs)</em>, also colloquially referred to as <em>bug bounty programs</em>. These programs offer rewards in exchange for responsibly disclosing vulnerabilities about your system, which may or may not come in cash form.<sup><a data-type="noteref" id="ch20fn9-marker" href="#ch20fn9">9</a></sup> Google’s first VRP, started in 2006, offered a T-shirt and a simple thank you message on our public-facing web page. Through reward programs, you can expand the hunt for security-related bugs outside of your immediate organization and engage with a larger number of security researchers.</p>
<p>Before starting a VRP, it’s a good idea to first cover the basics of finding and addressing regular security issues that thorough reviews and basic vulnerability scanning can find. Otherwise, you end up paying external people to find bugs that your own teams could have easily detected. This is not the intended purpose of VRPs. It also has the downside that <a href="https://sirdarckcat.blogspot.com/2017/12/the-optimal-way-for-rewarding.html">more than one researcher</a> may report the same issue to you.</p>
<p>Knowing how to set up a bug bounty program requires a little bit of legwork up front. If you choose to run a bug bounty program, you can follow these basic steps:</p>
<ol>
<li><p>Determine whether your organization is ready for this program.</p>
<ol type="a">
<li><p>Scope the areas of your system to target. For example, you might not be able to target corporate systems.</p></li>
<li><p>Determine payout levels and set aside funds for payouts.<sup><a data-type="noteref" id="ch20fn10-marker" href="#ch20fn10">10</a></sup></p></li>
</ol></li>
<li><p>Consider whether you want to run an in-house bug bounty program or hire an organization that specializes in these programs.</p></li>
<li><p>If running your own, set up a process for bug intake, triage, investigation, validation, follow-up, and fixes. In our experience, you can estimate this process to take approximately 40 hours for each serious issue, excluding fixes.</p></li>
<li><p>Define a process for making payments. Remember that reporters may be located all over the world, not just in your home country. You will need to work with your legal and finance teams to understand any constraints that may exist on your organization.</p></li>
<li><p>Launch, learn, and iterate.</p></li>
</ol>
<p>Every bug bounty program faces some likely challenges, including the following:</p>
<dl>
<dt>The need to fine-tune the firehose of issues being reported</dt>
<dd>Depending on your industry reputation, the attack surface, payout amounts, and the ease of finding bugs, you may be fielding an overwhelming number of reports. Understand up front what level of response will be required from your organization.</dd>
<dt>Poor report quality</dt>
<dd>A bug bounty program can become burdensome if most of your engineers are chasing down basic issues or nonissues. We have found this is especially true for web services, since many users have misconfigured browsers and “find” bugs that aren’t actually bugs. Security researchers are less likely to be in this pool, but sometimes it’s hard to discern a bug reporter’s qualifications up front.</dd>
<dt>Language barriers</dt>
<dd>A vulnerability researcher may not necessarily report a bug to you in your native language. Tools for language translation can be helpful here, or your organization may have someone who understands the language used by the reporter.</dd>
<dt>Vulnerability disclosure guidelines</dt>
<dd><p>The rules for disclosing vulnerabilities are not generally agreed upon. Should the researcher go public with what they know, and if so, when? How long should the researcher give your organization to fix the bug? What types of findings will be rewarded, and what types won’t? There are many differing opinions about the “right” methods to use here. Here are some suggestions for further reading:</p>
<ul>
<li><p>The Google security team has written a <a href="https://security.googleblog.com/2010/07/rebooting-responsible-disclosure-focus.html">blog post</a> on responsible disclosure.<sup><a data-type="noteref" id="ch20fn11-marker" href="#ch20fn11">11</a></sup></p></li>
<li><p>Project Zero, an internal vulnerability research team at Google, has also written a <a href="https://googleprojectzero.blogspot.com/2015/02/feedback-and-data-driven-updates-to.html">blog post</a> on data-driven updates to disclosure policy.</p></li>
<li><p>The Oulu University Secure Programming Group provides a useful collection of <a href="https://www.ee.oulu.fi/research/ouspg/Disclosure_tracking">vulnerability disclosure publications</a>.</p></li>
<li><p>The International Standards Organization (ISO) provides <a href="https://www.iso.org/standard/72311.html">recommendations</a> for vulnerability disclosure.</p></li>
</ul></dd>
</dl>
<p>Be prepared to address issues researchers report to you in a timely manner. Also be aware that they may find issues that have been actively exploited by a malicious actor for some period of time<a contenteditable="false" data-primary="" data-startref="ch20.html13" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html12" data-type="indexterm">&nbsp;</a>, in which case you may also have a security breach to address<a contenteditable="false" data-primary="" data-startref="ch20.html11" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html10" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html9" data-type="indexterm">&nbsp;</a>.<a contenteditable="false" data-primary="" data-startref="ch20.html3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch20.html2" data-type="indexterm">&nbsp;</a></p>
</section>
</section>
<section data-type="sect1" id="conclusion-id00019">
<h1>Conclusion</h1>
<p>Security and reliability are created by the quality or absence of processes and practices. People are the most important drivers of these processes and practices. Effective employees are able to collaborate across roles, departments, and cultural boundaries. We live in a world where the future is unknowable and our adversaries are unpredictable. At the end of the day, ensuring that everyone in your organization is responsible for security and reliability is the best defense!<a contenteditable="false" data-primary="" data-startref="ch20.html0" data-type="indexterm">&nbsp;</a></p>
</section>
</section>
</body>
</html>
<div data-type="footnotes">
<p data-type="footnote" id="ch20fn1"><sup><a href="#ch20fn1-marker">1</a></sup>The development arc described in <a class="orm:hideurl" href="https://landing.google.com/sre/workbook/chapters/engagement-model/">Chapter 18 of the SRE workbook</a> demonstrates the value that an experienced SRE offers throughout the entire product lifecycle.</p>
<p data-type="footnote" id="ch20fn2"><sup><a href="#ch20fn2-marker">2</a></sup>The Sarbanes-Oxley Act of 2002 (the Public Company Accounting Reform and Investor Protection Act) sets standards for public US companies regarding their accounting practices, and includes information security topics. The Payment Card Industry Data Security Standard sets minimum guidelines around protecting credit card information; compliance is required for anyone doing payment processing of this kind. The General Data Protection Regulation is an EU regulation concerned with the handling of personal data.</p>
<p data-type="footnote" id="ch20fn3"><sup><a href="#ch20fn3-marker">3</a></sup>A few notable cases of organizations that went out of business or filed for bankruptcy after breaches are <a href="https://www.infoworld.com/article/2608076/murder-in-the-amazon-cloud.html">Code Spaces</a> and the <a href="https://news.bloomberglaw.com/health-law-and-business/american-medical-collection-agency-parent-files-for-bankruptcy">American Medical Collection Agency</a>.</p>
<p data-type="footnote" id="ch20fn4"><sup><a href="#ch20fn4-marker">4</a></sup>In the US, executives and boards of directors are increasingly being held accountable for security in their organizations. The Concord Law School at Purdue University has written a good <a href="https://www.concordlawschool.edu/blog/news/liability-corporate-boards-data-breaches/">article</a> on this trend.</p>
<p data-type="footnote" id="ch20fn5"><sup><a href="#ch20fn5-marker">5</a></sup>See <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/communication-and-collaboration/">Chapter 31 in the SRE book</a>.</p>
<p data-type="footnote" id="ch20fn6"><sup><a href="#ch20fn6-marker">6</a></sup>This color scheme is derived from <a href="https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dodm/857001m.pdf">the US military</a>.</p>
<p data-type="footnote" id="ch20fn7"><sup><a href="#ch20fn7-marker">7</a></sup>For more on Purple Teams, see Brotherston, Lee, and Amanda Berlin. 2017. <a class="orm:hideurl" href="http://shop.oreilly.com/product/0636920051671.do"><em>Defensive Security Handbook: Best Practices for Securing Infrastructure</em></a>. Sebastopol, CA: O’Reilly Media.</p>
<p data-type="footnote" id="ch20fn8"><sup><a href="#ch20fn8-marker">8</a></sup>Do so by building on a culture of blameless postmortems, as described in <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/postmortem-culture/">Chapter 15 of the SRE book</a>.</p>
<p data-type="footnote" id="ch20fn9"><sup><a href="#ch20fn9-marker">9</a></sup>See Google researcher sirdarckcat’s <a href="https://sirdarckcat.blogspot.com/2015/09/not-about-money.html">blog post on rewards</a> for a more philosophical outlook.</p>
<p data-type="footnote" id="ch20fn10"><sup><a href="#ch20fn10-marker">10</a></sup>For further reading, see sirdarckcat’s post about <a href="https://sirdarckcat.blogspot.com/2016/12/vulnerability-pricing.html">vulnerability pricing</a>.</p>
<p data-type="footnote" id="ch20fn11"><sup><a href="#ch20fn11-marker">11</a></sup>sirdarckcat has also written a post about <a href="https://sirdarckcat.blogspot.com/2017/02/vulnerability-disclosure-in-era-of.html">vulnerability disclosure</a>.</p>
</div>
