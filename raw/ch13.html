<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Building Secure and Reliable Systems</title>
  <link rel="stylesheet" type="text/css" href="theme/html/html.css">
</head>
<body data-type="book">
<section xmlns="http://www.w3.org/1999/xhtml" data-type="chapter" id="onethree_testing_code">
<h1>Testing Code</h1>

<p class="byline">By Phil Ames and Franjo Ivančić</p>
<p class="byline cont">with Vera Haas and Jen Barnason</p>

<aside data-type="sidebar" id="a_reliable_system_is_resilient_to_failu">
<p><a contenteditable="false" data-primary="testing (code)" data-type="indexterm" id="ch13.html0">&nbsp;</a>A reliable system is resilient to failures and meets its documented service level objectives, which may also include security guarantees. Robust software testing and analysis are useful aids in mitigating failure risks, and should be a particular focus during the project implementation phase.</p>
<p>In this chapter, we discuss several approaches to testing, including unit and integration testing. We also cover additional security deep-dive topics like static and dynamic program analysis and fuzz testing, which can help strengthen your software’s resilience against the inputs it encounters.</p>
</aside>
<p>No matter how careful the engineers developing your software are, some mistakes and overlooked edge cases are inevitable. Unexpected input combinations may cause data corruption or result in availability issues like the “Query of Death” example in <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/">Chapter 22 of the SRE book</a>. Coding errors can cause security problems like buffer overflows and cross-site scripting vulnerabilities. Put simply, there are many ways software is prone to failure in the real world.</p>
<p>The techniques discussed in this chapter, used in different stages and contexts of software development, have a variety of cost–benefit profiles.<sup><a data-type="noteref" id="ch13fn1-marker" href="#ch13fn1">1</a></sup> <a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-type="indexterm">&nbsp;</a>For example, <em>fuzzing</em>—sending random requests to a system—can help you harden that system in terms of both security and reliability. This technique can potentially help you catch <span class="keep-together">information</span> leaks and reduce serving errors by exposing the service to a multitude of edge cases. To identify potential bugs in systems that you can’t patch easily and quickly, you’ll likely need to perform thorough up-front testing.</p>
<section data-type="sect1" id="unit_testing">
<h1>Unit Testing</h1>
<p><a contenteditable="false" data-primary="testing (code)" data-secondary="unit testing" data-type="indexterm" id="ch13.html1">&nbsp;</a><a contenteditable="false" data-primary="unit testing" data-type="indexterm" id="ch13.html2">&nbsp;</a><em>Unit testing</em> can increase system security and reliability by pinpointing a wide range of bugs in individual software components before a release. This technique involves breaking software components into smaller, self-contained “units” that have no external dependencies, and then testing each unit. Unit tests consist of code that exercises a given unit with different inputs selected by the engineer writing the test. Popular unit test frameworks are available for many languages; systems based on the <a href="https://en.wikipedia.org/wiki/XUnit">xUnit</a> architecture are very common.</p>
<p>Frameworks following the xUnit paradigm allow common setup and teardown code to execute with each individual test method. These frameworks also define roles and responsibilities for individual testing framework components that help standardize the test result format. That way, other systems have detailed information about what exactly went wrong. Popular examples include JUnit for Java, GoogleTest for C++, go2xunit for Golang, and the built-in <code>unittest</code> module in Python.</p>
<p><a data-type="xref" href="#example_onethree_onedot_unit_test_for_a">example_onethree_onedot_unit_test_for_a</a> is a simple <a href="https://github.com/google/googletest/blob/master/googletest/samples/sample1_unittest.cc#L124">unit test</a> written using the GoogleTest framework.</p>
<div data-type="example" id="example_onethree_onedot_unit_test_for_a">
<h5>Unit test for a function that checks whether the provided argument is a prime number, written using the GoogleTest framework</h5>
<pre data-type="programlisting">TEST(IsPrimeTest, Trivial) {
  EXPECT_FALSE(IsPrime(0));
  EXPECT_FALSE(IsPrime(1));
  EXPECT_TRUE(IsPrime(2));
  EXPECT_TRUE(IsPrime(3));
}</pre>
</div>
<p>Unit tests typically run locally as part of engineering workflows to provide fast feedback to developers before they submit changes to the codebase. <a contenteditable="false" data-primary="continuous integration/continuous deployment (CI/CD)" data-secondary="unit tests" data-type="indexterm">&nbsp;</a>In continuous integration/continuous delivery (CI/CD) pipelines, unit tests often run before a commit is merged into a repository’s mainline branch. This practice attempts to prevent code changes that break behavior that other teams rely on.</p>
<section data-type="sect2" id="writing_effective_unit_tests">
<h2>Writing Effective Unit Tests</h2>
<p><a contenteditable="false" data-primary="unit testing" data-secondary="writing effective unit tests" data-type="indexterm">&nbsp;</a>The quality and comprehensiveness of unit tests can significantly impact the robustness of your software. Unit tests should be fast and reliable to give engineers immediate feedback on whether a change has broken expected behavior. By writing and maintaining unit tests, you can ensure that as engineers add new features and code, they do not break existing behavior covered by the relevant tests. As discussed in <a data-type="xref" href="#design_for_recovery">Chapter 9</a>, your tests should also be hermetic—if a test can’t repeatedly produce the same results in an isolated environment, you can’t necessarily rely on the test results.</p>
<p>Consider a system that manages the amount of storage bytes a team can use in a given datacenter or region. Suppose that the system allows teams to request additional quota if the datacenter has available unallocated bytes. A simple unit test might involve validating requests for quota in a set of imaginary clusters partially occupied by imaginary teams, rejecting requests that would exceed the available storage capacity. Security-focused unit tests might check how requests involving negative amounts of bytes are handled, or how the code handles capacity overflows for large transfers that result in quota values near the limit of the variable types used to represent them. Another unit test might check whether the system returns an appropriate error message when sent malicious or malformed input.</p>
<p>It’s often useful to test the same code with different parameters or environmental data, such as the initial starting quota usages in our example. To minimize the amount of duplicated code, unit test frameworks or languages often provide a way to invoke the same test with different parameters. This approach helps reduce duplicate boilerplate code, which can make refactoring efforts less tedious.</p>
</section>
<section data-type="sect2" id="when_to_write_unit_tests">
<h2>When to Write Unit Tests</h2>
<p><a contenteditable="false" data-primary="unit testing" data-secondary="when to write unit tests" data-type="indexterm">&nbsp;</a>A common strategy is to write tests shortly after writing the code, using the tests to verify that the code performs as expected. These tests typically accompany the new code in the same commit, and often encompass the cases that the engineer writing the code checked manually. For instance, our example storage management application might require that “Only billing administrators for the group that owns the service can request more quota.” You can translate this type of requirement into several unit tests.</p>
<p>In organizations that practice code review, a peer reviewer can double-check the tests to ensure they’re sufficiently robust to maintain the quality of the codebase. For example, a reviewer may notice that although new tests accompany a change, the tests may pass even if the new code is removed or inactive. If a reviewer can replace a statement like <code>if (condition_1 || condition_2)</code> with <code>if (false)</code> or <code>if (true)</code> in the new code, and none of the new tests fail, then the test may have overlooked important test cases. <a contenteditable="false" data-primary="mutation testing" data-type="indexterm">&nbsp;</a>For more information about Google’s experience with automating this kind of <em>mutation testing</em>, see Petrović and Ivanković (2018).<sup><a data-type="noteref" id="ch13fn2-marker" href="#ch13fn2">2</a></sup></p>
<p>Instead of writing tests <em>after</em> writing code, test-driven development (TDD) methodologies encourage engineers to write unit tests based on established requirements and expected behaviors <em>before</em> writing code. When testing new features or bug fixes, the tests will fail until the behavior is completely implemented. Once a feature is implemented and the tests pass, engineers progress to the next feature, where the process repeats.</p>
<p>For existing projects that weren’t built using TDD models, it is common to slowly integrate and improve test coverage in response to bug reports or proactive efforts to increase confidence in a system. But even once you achieve full coverage, your project isn’t necessarily bug-free. Unknown edge cases or sparsely implemented error handling can still cause incorrect behavior.</p>
<p>You can also write unit tests in response to internal manual testing or code review efforts. You might write these tests during standard development and review practices, or during milestones like a security review before a launch. New unit tests can verify that a proposed bug fix works as expected, and that later refactoring won’t reintroduce the same bug. This type of testing is particularly important if the code is hard to reason about and potential bugs impact security—for example, when writing access control checks in a system with a complicated permission model.</p>
<div data-type="note">
<p>In the interest of covering as many scenarios as possible, you’ll often spend more time writing tests than writing the code being tested—especially when dealing with nontrivial systems. This extra time pays off in the long run, since early testing yields a higher-quality codebase with fewer edge cases to debug.</p>
</div>
</section>
<section data-type="sect2" id="how_unit_testing_affects_code">
<h2>How Unit Testing Affects Code</h2>
<p><a contenteditable="false" data-primary="unit testing" data-secondary="effect on code" data-type="indexterm" id="ch13.html3">&nbsp;</a>To improve the comprehensiveness of your tests, you may need to design new code to include testing provisions, or refactor older code to make it more testable. <a contenteditable="false" data-primary="refactoring" data-type="indexterm">&nbsp;</a>Typically, refactoring involves providing a way to intercept calls to external systems. Using that introspection ability, you can test code in a variety of ways—for example, to verify that the code invokes the interceptor the correct number of times, or with the correct arguments.</p>
<p>Consider how you might test a piece of code that opens tickets in a remote issue tracker when certain conditions are met. Creating a real ticket every time the unit test runs would generate unnecessary noise. Even worse, this testing strategy may fail randomly if the issue tracker system is unavailable, violating the goal of quick, reliable test results.</p>
<p>To refactor this code, you could remove direct calls to the issue tracker service and replace those calls with an abstraction—for example, an interface for an <code>IssueTrackerService</code> object. The implementation for testing could record data when it receives calls such as “Create an issue,” and the test could inspect that metadata to make a pass or fail conclusion. By contrast, the production implementation would connect to remote systems and call the exposed API methods.</p>
<p>This refactor dramatically reduces the “flakiness” of a test that depends on real-world systems. Because they rely on behavior that isn’t guaranteed—like an external dependency, or the order of elements when retrieving items from some container types—flaky tests are often more of a nuisance than a help. Try to fix flaky tests as they arise; otherwise, developers may get in the habit of ignoring test results when checking in changes.</p>
<div data-type="note">
<p>These abstractions and their corresponding implementations are called <em>mocks</em>, <em>stubs</em>, or <em>fakes</em>. Engineers sometimes use these words interchangeably, despite the fact that the concepts vary in implementation complexity and features, so it’s important to ensure everyone at your organization uses consistent vocabulary. If you practice code review or use style guides, you can help reduce confusion by providing definitions that teams can align around.</p>
</div>
<p>It’s easy to fall into the trap of overabstraction, where tests assert mechanical facts about the order of function calls or their arguments. Overly abstracted tests often don’t provide much value, as they tend to “test” the language’s control flow implementation rather than the behavior of the systems you care about.</p>
<p>If you have to completely rewrite your tests every time a method changes, you may need to rethink the tests—or even the architecture of the system itself. To help avoid constant test rewrites, you might consider asking engineers familiar with the service to provide suitable fake implementations for any nontrivial testing needs. This solution is advantageous to both the team responsible for the system and the engineers testing the code: the team that owns the abstraction can ensure it tracks the feature set of the service as it evolves, and the team using the abstraction now has a more realistic component to use in its tests.<sup><a data-type="noteref" id="ch13fn3-marker" href="#ch13fn3">3</a></sup></p>
<aside data-type="sidebar" id="correctness_validation">
<h5>Correctness Validation</h5>
<p><a contenteditable="false" data-primary="testing (code)" data-secondary="correctness validation" data-type="indexterm">&nbsp;</a>Carefully designed test suites can evaluate the correctness of different pieces of software that perform the same task. This functionality can be very useful in specialized domains. For example, compilers often have test suites that focus on esoteric corner cases of programming languages. One such example is the GNU C Compiler’s <a href="https://github.com/gcc-mirror/gcc/tree/master/gcc/testsuite/gcc.c-torture">“torture test” suite</a>.</p>
<p><a contenteditable="false" data-primary="Wycheproof" data-type="indexterm">&nbsp;</a>Another example is the <a href="https://github.com/google/wycheproof">Wycheproof</a> set of test vectors, which is designed to validate the correctness of cryptographic algorithm implementations against certain known attacks. <a contenteditable="false" data-primary="Java Cryptography Architecture (JCA)" data-type="indexterm">&nbsp;</a>Wycheproof takes advantage of Java’s standardized interface—the Java Cryptography Architecture (JCA)—to access cryptography functions. Authors of cryptography software write implementations against JCA, and JCA’s cryptography providers handle calls to cryptography functions. The provided test vectors can also be used with other programming languages.</p>
<p>There are also test suites that aim to exercise every rule in an RFC for parsing a particular media format. Engineers attempting to design drop-in replacement parsers can rely on such tests to ensure that the old and new implementations are compatible and produce equivalent observable outputs.<a contenteditable="false" data-primary="" data-startref="ch13.html2" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html1" data-type="indexterm">&nbsp;</a></p>
</aside>
</section>
</section>
<section data-type="sect1" id="integration_testing">
<h1>Integration Testing</h1>
<p><a contenteditable="false" data-primary="integration testing" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="testing (code)" data-secondary="integration testing" data-type="indexterm">&nbsp;</a><em>Integration testing</em> moves beyond individual units and abstractions, replacing fake or stubbed-out implementations of abstractions like databases or network services with real implementations. As a result, integration tests exercise more complete code paths. Because you must initialize and configure these other dependencies, integration testing may be slower and flakier than unit testing—to execute the test, this approach incorporates real-world variables like network latency as services communicate end-to-end. As you move from testing individual low-level units of code to testing how they interact when composed together, the net result is a higher degree of confidence that the system is behaving as expected.</p>
<p>Integration testing takes different shapes, which are determined by the complexity of the dependencies they address. When the dependencies that integration testing needs are relatively simple, an integration test may look like a base class that sets up a few shared dependencies (for example, a database in a preconfigured state) from which other tests extend. As services grow in complexity, integration tests can become far more complex, requiring supervising systems to orchestrate the initialization or setup of dependencies to support the test. Google has teams focused exclusively on infrastructure that enables standardized integration test setup for common infrastructure services. For organizations using a continuous build and delivery system like <a href="https://jenkins.io">Jenkins</a>, integration tests may run either alongside or separately from unit tests, depending on the size of the codebase and the number of available tests in a project.</p>
<div data-type="note">
<p>As you build integration tests, keep the principles discussed in <a data-type="xref" href="#design_for_least_privilege">Chapter 5</a> in mind: make sure the data and systems access requirements of the tests don’t introduce security risks. It can be tempting to mirror actual databases into test environments since the databases provide a rich set of real-world data, but you should avoid this anti-pattern because they may contain sensitive data that will be available to anyone running tests that use those databases. Such an implementation is inconsistent with the principle of least privilege and may pose a security risk. Instead, you can seed these systems with nonsensitive test data. This approach also makes it easy to wipe test environments to a known clean state, reducing the likelihood of integration test flakiness.</p>
</div>
<section data-type="sect2" id="writing_effective_integration_tests">
<h2>Writing Effective Integration Tests</h2>
<p>Like unit tests, integration tests may be influenced by design choices in your code. To continue with our earlier example of an issue tracker that files tickets, a unit test mock may simply assert that the method was invoked to file a ticket with the remote service. An integration test would more likely use a real client library. Rather than creating spurious bugs in production, the integration test would communicate with a QA endpoint. Test cases would exercise the application logic with inputs that cause calls to the QA instance. Supervising logic could then query the QA instance to verify that externally visible actions took place successfully from an end-to-end perspective.</p>
<p>Understanding why integration tests fail when all unit tests pass can require a lot of time and energy. Good logging at key logical junctures of your integration tests can help you debug and understand where breakdowns occur. Bear in mind too that because integration tests go beyond individual units by examining interactions between components, they can tell you only a limited amount about how well those units will conform to your expectations in other scenarios. This is one of the many reasons using each type of testing in your development lifecycle adds value—one form of testing is often not a substitute for another.</p>
</section>
</section>
<section data-type="sect1" id="dynamic_program_analysis">
<h1 class="dive">Dynamic Program Analysis</h1>
<p><a contenteditable="false" data-primary="dynamic program analysis" data-type="indexterm" id="ch13.html4">&nbsp;</a><a contenteditable="false" data-primary="program analysis" data-secondary="dynamic" data-type="indexterm" id="ch13.html5">&nbsp;</a><a contenteditable="false" data-primary="testing (code)" data-secondary="dynamic program analysis" data-type="indexterm" id="ch13.html6">&nbsp;</a><em>Program analysis</em> allows users to carry out a number of useful actions—for example, performance profiling, checking for security-related correctness, code coverage reporting, and dead code elimination. As discussed later in this chapter, you can perform program analysis <em>statically</em> to investigate software without executing it. Here, we focus on <em>dynamic</em> approaches. Dynamic program analysis analyzes software by running programs, potentially in virtualized or emulated environments, for purposes beyond just testing.</p>
<p>Performance profilers (which are used to find performance issues in programs) and code coverage report generators are the best-known types of dynamic analysis. The previous chapter introduced the dynamic program analysis tool <a href="http://www.valgrind.org">Valgrind</a>, which provides a virtual machine and various tools to interpret a binary and check whether an execution exhibits various common bugs. This section focuses on dynamic analysis approaches that rely on compiler support (often called <em>instrumentation</em>) to detect memory-related errors.</p>
<p>Compilers and dynamic program analysis tools let you configure instrumentation to collect runtime statistics on the binaries that the compilers produce, such as performance profiling information, code coverage information, and profile-based optimizations. The compiler inserts additional instructions and callbacks to a backend runtime library that surfaces and collects the relevant information when the binary is executed. Here, we focus on security-relevant memory misuse bugs for C/C++ <span class="keep-together">programs</span>.</p>
<p><a contenteditable="false" data-primary="Google Sanitizers" data-type="indexterm">&nbsp;</a>The Google Sanitizers suite provides compilation-based dynamic analysis tools. They were initially developed as part of the <a href="https://llvm.org">LLVM</a> compiler infrastructure to capture common programming mistakes, and are now supported by GCC and other compilers, as well. <a contenteditable="false" data-primary="AddressSanitizer (ASan)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="ASan (AddressSanitizer)" data-type="indexterm">&nbsp;</a>For example, <a href="https://clang.llvm.org/docs/AddressSanitizer.html">AddressSanitizer (ASan)</a> finds a number of common memory-related bugs, such as out-of-bounds memory accesses, in C/C++ programs. Other popular sanitizers include the <span class="keep-together">following</span>:</p>
<dl>
<dt><a href="https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html">UndefinedBehaviorSanitizer</a></dt>
<dd>Performs runtime flagging of undefined behavior</dd>
<dt><a href="https://clang.llvm.org/docs/ThreadSanitizer.html">ThreadSanitizer</a></dt>
<dd>Detects race conditions</dd>
<dt><a href="https://clang.llvm.org/docs/MemorySanitizer.html">MemorySanitizer</a></dt>
<dd>Detects reading of uninitialized memory</dd>
<dt><a href="https://clang.llvm.org/docs/LeakSanitizer.html">LeakSanitizer</a></dt>
<dd>Detects memory leaks and other types of leaks</dd>
</dl>
<p>As new hardware features allow tagging of memory addresses, there are <a href="https://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html">proposals</a> to use those new features to further improve the performance of ASan.</p>
<p>ASan provides fast performance by building a custom, instrumented binary of the program under analysis. During compilation, ASan adds certain instructions to make callbacks into the provided sanitizer runtime. The runtime maintains metadata about the program execution—for example, which memory addresses are valid to access. ASan uses a shadow memory to record whether a given byte is safe for the program to access, and uses compiler-inserted instructions to check the shadow memory when the program tries to read or write that byte. It also provides custom memory allocation and deallocation (<code>malloc</code> and <code>free</code>) implementations. For example, the <code>malloc</code> function allocates additional memory immediately before and after the returned requested memory region. This creates a buffer memory region that allows ASan to easily report buffer overflows and underflows with precise information about what went wrong and where. <a contenteditable="false" data-primary="poisoned regions" data-type="indexterm">&nbsp;</a>To do so, ASan marks these regions (also called <em>red zones</em>) as <em>poisoned</em>. Similarly, ASan marks memory that was freed as poisoned, allowing you to catch use-after-free bugs easily.</p>
<p>The following example illustrates a simple run of ASan, using the Clang compiler. The shell commands instrument and run a particular input file with a use-after-free bug, which occurs when a memory address belonging to a previously deallocated memory region is read. A security exploit can use this type of access as a building block. The option <code>-fsanitize=address</code> turns on the ASan instrumentation:</p>
<pre data-type="programlisting">$ <strong><span>cat -n use-after-free.c</span></strong>
 1  #include &lt;stdlib.h&gt;
 2  int main() {
 3    char *x = (char*)calloc(10, sizeof(char));
 4    free(x);
 5    return x[5];
 6  }

$ <strong><span>clang -fsanitize=address -O1 -fno-omit-frame-pointer -g use-after-free.c</span></strong></pre>
<p>After the compilation finishes, we can see the error report ASan produces when executing the generated binary. (For the sake of brevity, we’ve omitted the full ASan error message.) Note that ASan allows error reports to indicate the source file information, such as line numbers, by using the LLVM symbolizer, as described in the <a href="https://clang.llvm.org/docs/AddressSanitizer.html#symbolizing-the-reports">“Symbolizing the Reports” section</a> of the Clang documentation. As you can see in the output report, ASan finds a 1-byte use-after-free read access (emphasis added). The error message includes information for the original allocation, the deallocation, and the subsequent illegal use:</p>
<pre data-type="programlisting">% <strong><span>./a.out</span></strong>
=================================================================
==142161==ERROR: AddressSanitizer: heap-use-after-free on address 0x602000000015 
at pc 0x00000050b550 bp 0x7ffc5a603f70 sp 0x7ffc5a603f68
READ of size 1 at 0x602000000015 thread T0
    #0 0x50b54f in main use-after-free.c:5:10
    #1 0x7f89ddd6452a in __libc_start_main 
    #2 0x41c049 in _start 

0x602000000015 is located 5 bytes inside of 10-byte region [0x602000000010,0x60200000001a)
freed by thread T0 here:
    #0 0x4d14e8 in free 
    #1 0x50b51f in main use-after-free.c:4:3
    #2 0x7f89ddd6452a in __libc_start_main 

previously allocated by thread T0 here:
    #0 0x4d18a8 in calloc 
    #1 0x50b514 in main use-after-free.c:3:20
    #2 0x7f89ddd6452a in __libc_start_main 

SUMMARY: AddressSanitizer: heap-use-after-free use-after-free.c:5:10 in main
[...]
==142161==ABORTING</pre>
<aside data-type="sidebar" id="performance_tradeoffs_in_dynamic_progra">
<h5>Performance Tradeoffs in Dynamic Program Analysis</h5>
<p>Dynamic program analysis tools like sanitizers provide developers with useful feedback about correctness and other dimensions, such as performance and code coverage. This feedback comes at a performance cost: the compiler-instrumented binaries can be orders of magnitude slower than the native binaries. As a result, many projects are adding sanitizer-enhanced pipelines to their existing CI/CD systems, but running those pipelines less frequently—for example, nightly. This practice may catch otherwise hard-to-identify bugs caused by memory corruption issues. Other program analysis–based CI/CD-enabled pipelines collect additional developer signals, such as nightly code coverage metrics. Over time, you can use these signals to gauge various code health metrics.<a contenteditable="false" data-primary="" data-startref="ch13.html6" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html5" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html4" data-type="indexterm">&nbsp;</a></p>
</aside>
</section>
<section data-type="sect1" id="fuzz_testing">
<h1 class="dive">Fuzz Testing</h1>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-type="indexterm" id="ch13.html7">&nbsp;</a><a contenteditable="false" data-primary="testing (code)" data-secondary="fuzz testing" data-type="indexterm" id="ch13.html8">&nbsp;</a><em>Fuzz testing</em> (often referred to as <em>fuzzing</em>) is a technique that complements the previously mentioned testing strategies. Fuzzing involves using a <em>fuzz engine</em> (or <em>fuzzer</em>) to generate large numbers of candidate inputs that are then passed through a <em>fuzz driver</em> to the <em>fuzz target</em> (the code that processes the inputs). The fuzzer then analyzes how the system handles the input. Complex inputs handled by all kinds of software are popular targets for fuzzing—for example, file parsers, compression algorithm implementations, network protocol implementations, and audio codecs.</p>
<aside data-type="sidebar" id="security_and_reliability_benefits_of_fu">
<h5>Security and Reliability Benefits of Fuzzing</h5>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="security/reliability benefits" data-type="indexterm">&nbsp;</a>One motivation for fuzzing is to find bugs like memory corruption that have security implications. Fuzzing can also identify inputs that cause runtime exceptions that may cause a cascading denial of service in languages like Java and Go.</p>
<p>Fuzzing can be useful for testing service resilience. Google performs regular manual and automated disaster recovery testing exercises. The automated exercises are valuable for finding regressions in a controlled manner. If a system crashes when given malformed input, or if the system returns an error when using a special character, the results can have serious effects on the error budget<sup><a data-type="noteref" id="ch13fn4-marker" href="#ch13fn4">4</a></sup> and can result in customer dissatisfaction. <a contenteditable="false" data-primary="chaos engineering" data-type="indexterm">&nbsp;</a>Practices referred to as <a href="https://principlesofchaos.org/?lang=ENcontent"><em>chaos engineering</em></a> help automatically identify such weaknesses by injecting different kinds of faults like latency and service failure into a system. <a contenteditable="false" data-primary="Netflix" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Simian Army" data-type="indexterm">&nbsp;</a>Netflix’s <a href="https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116">Simian Army</a> is an early example of a suite of tools that can perform this testing. Some of its components are now integrated into other release engineering tools, like <a href="https://www.spinnakersummit.com/blog/the-101-on-chaos-monkey-and-spinnaker">Spinnaker</a>.</p>
</aside>
<p>You can also use fuzzing to evaluate different implementations of the same functionality. For example, if you are considering moving from library A to library B, a fuzzer can generate inputs, pass them to each library for processing, and compare the results. The fuzzer can report any nonmatching result as a “crash,” which can help engineers determine what subtle behavior changes may result. This crash-on-different-outputs action is typically implemented as part of the fuzz driver, as seen in OpenSSL’s <a href="https://github.com/openssl/openssl/blob/OpenSSL_1_1_0-stable/fuzz/bignum.c">BigNum fuzzer</a>.<sup><a data-type="noteref" id="ch13fn5-marker" href="#ch13fn5">5</a></sup></p>
<p>Since fuzzing can execute indefinitely, it’s not feasible to block every commit on the results of an extended test. This means that when the fuzzer finds a bug, that bug may already be checked in. Ideally, other testing or analysis strategies will have prevented the bug in the first place, so fuzzing acts as a complement by generating test cases that engineers may not have considered. As an added benefit, another unit test can use the generated input samples that identify bugs in the fuzz target to ensure that later changes don’t regress the fix.</p>
<section data-type="sect2" id="how_fuzz_engines_work">
<h2>How Fuzz Engines Work</h2>
<p><a contenteditable="false" data-primary="fuzz engines" data-type="indexterm" id="ch13.html9">&nbsp;</a><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="how fuzz engines work" data-type="indexterm" id="ch13.html10">&nbsp;</a>Fuzz engines can vary in complexity and sophistication. <a contenteditable="false" data-primary="dumb fuzzing" data-type="indexterm">&nbsp;</a>At the low end of the spectrum, a technique often referred to as <em>dumb fuzzing</em> simply reads bytes from a random number generator and passes them to the fuzz target in an attempt to find bugs. Fuzz engines have grown increasingly smart through integration with compiler toolchains. They can now generate more interesting and meaningful samples by taking advantage of the compiler instrumentation features discussed earlier. It is considered a good industry practice to use as many fuzz engines as you can integrate into your build toolchain, and to monitor metrics like the percentage of code covered. If code coverage plateaus at some point, it’s usually worth investigating why the fuzzer can’t reach other areas.</p>
<p><a contenteditable="false" data-primary="dictionaries, fuzz engines and" data-type="indexterm">&nbsp;</a>Some fuzz engines accept dictionaries of interesting keywords from the specifications or grammars of well-specified protocols, languages, and formats (like HTTP, SQL, and JSON). The fuzz engine can then generate input that’s likely to be accepted by the program under test, since the input may simply be rejected by generated parser code if it contains illegal keywords. Providing a dictionary increases the likelihood of reaching the code you actually want to test through fuzzing. Otherwise, you may end up exercising code that rejects input based on invalid tokens and never finds any interesting bugs.</p>
<p><a contenteditable="false" data-primary="Peach Fuzzer" data-type="indexterm">&nbsp;</a>Fuzz engines like <a href="http://www.peach.tech/resources/peachcommunity/">Peach Fuzzer</a> allow a fuzz driver author to programmatically define the format of the input and the expected relationships between fields, so the fuzz engine can generate test cases that violate those relationships. <a contenteditable="false" data-primary="seed corpus" data-type="indexterm">&nbsp;</a>Fuzz engines also commonly accept a set of sample input files, referred to as a <em>seed corpus</em>, that are representative of what the code being fuzzed expects. The fuzz engine then mutates these seed inputs, in addition to carrying out any other supported input generation strategies. Some software packages come with sample files (such as MP3s for audio libraries or JPEGs for image processing) as part of their existing test suites—these sample files are great candidates for a seed corpus. Otherwise, you can curate a seed corpus from real-world or hand-generated files. Security researchers also publish seed corpora for popular file formats, such as those provided by the following:</p>
<ul>
<li><p><a href="https://github.com/google/oss-fuzz/blob/master/docs/advanced-topics/corpora.md">OSS-Fuzz</a></p></li>
<li><p><a href="https://files.fuzzing-project.org/">The Fuzzing Project</a></p></li>
<li><p><a href="http://lcamtuf.coredump.cx/afl/demo/">American Fuzzy Lop (AFL)</a></p></li>
</ul>
<p>In recent years, improvements to compiler toolchains have resulted in significant advancements toward making smarter fuzz engines. <a contenteditable="false" data-primary="C++" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="LLVM Clang" data-type="indexterm">&nbsp;</a>For C/C++, compilers such as LLVM Clang can instrument the code (as discussed earlier) to allow the fuzz engine to observe what code is executed while processing a specific sample input. When the fuzz engine finds a new code path, it preserves the samples that prompted the code path and uses them to generate future samples. Other languages or fuzz engines may require a specific compiler—such as afl-gcc for <a href="https://github.com/google/AFL">AFL</a> or go-fuzz-build for the <a href="https://github.com/dvyukov/go-fuzz">go-fuzz engine</a>—to properly trace the execution paths to increase code coverage.</p>
<p>When a fuzz engine generates an input that caused a crash in a sanitizer-instrumented code path, it records the input along with metadata extracted from the program in the crashed state. This metadata can include information such as a stack trace indicating what line of code caused the crash, or the process’s memory layout at the time. This information provides engineers with details about the cause of the crash, which can help them understand its nature, prepare fixes, or prioritize bugs. For example, when an organization is considering how to prioritize fixes for different types of issues, a memory read access violation may be considered less critical than a write access violation. Such prioritization contributes to a culture of security and reliability (see <a data-type="xref" href="#twoone_building_a_culture_of_security_a">Chapter 21</a>).</p>
<p>The way your program reacts when the fuzz engine provokes a potential bug depends on a wide variety of circumstances. A fuzz engine is most effective at detecting bugs if encountering them prompts consistent and well-defined events—for example, receiving a signal or executing a specific function when memory corruption or undefined behavior occurs. These functions can explicitly signal the fuzz engine when the system reaches a particular error state. Many of the sanitizers mentioned earlier work this way.</p>
<p>Some fuzz engines also allow you to set an upper time bound for processing a particular generated input. If, for example, a deadlock or infinite loop causes an input to exceed the time limit, the fuzzer categorizes the sample as “crashing.” It also saves that sample for further investigation so development teams can prevent DoS issues that might render the service unavailable.</p>
<aside data-type="sidebar" id="quotation_markknown_safequotation_mark">
<h5>“Known Safe” Functions</h5>
<p><a contenteditable="false" data-primary="known safe functions" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="&quot;known safe&quot; functions" data-secondary-sortas="known safe" data-type="indexterm">&nbsp;</a>In some cases, benign bugs may impede fuzzing. This can happen with code that has undefined behavior semantics, notably in C/C++ programs. For example, C++ does not define what happens in the case of <a href="https://en.cppreference.com/w/cpp/language/ub">signed integer overflow</a>. Suppose a function has an easily reachable signed integer that causes an <code>UndefinedBehaviorSanitizer</code> crash. If the value is discarded or unused in contexts like determining allocation sizes or indexing, an overflow doesn’t have any security or reliability consequences. However, this “shallow crash” may prevent fuzzing from reaching more interesting code. If it’s not feasible to patch the code by moving to unsigned types or clamping the bounds, you can manually annotate functions as “known safe” to disable specific sanitizers—for example,  <code>__attribute__((no_sanitize("undefined")))</code> in order to uncover deeper bugs. Because this approach can lead to false negatives, add manual annotations only after careful review and <span class="keep-together">consideration</span>.</p>
</aside>
<p><a contenteditable="false" data-primary="Heartbleed security bug" data-type="indexterm">&nbsp;</a>The Heartbleed bug (CVE-2014-0160) that caused web servers to leak memory (including memory containing TLS certificates or cookies) can be identified relatively quickly by fuzzing with the right fuzz driver and sanitizer. Google’s <a href="https://github.com/google/fuzzer-test-suite">fuzzer-test-suite GitHub repository</a> contains an example Dockerfile that demonstrates successful identification of the bug. Here is an excerpt of the ASan report for the Heartbleed bug, provoked by the <code>__asan_memcpy</code> function call that the sanitizer compiler plug-in inserted (emphasis added):</p>
<pre class="small pagebreak-before" data-type="programlisting">==19==ERROR: AddressSanitizer: <strong><span>heap-buffer-overflow</span></strong> on address 0x629000009748 at pc 
0x0000004e59c9 bp 0x7ffe3a541360 sp 0x7ffe3a540b10
<strong><span>READ of size 65535</span></strong> at 0x629000009748 thread T0
    #0 0x4e59c8 in __asan_memcpy /tmp/final/llvm.src/projects/compiler-rt/lib/asan/asan_interceptors_memintrinsics.cc:23:3
    #1 0x522e88 in <strong><span>tls1_process_heartbeat</span></strong> /root/heartbleed/BUILD/ssl/t1_lib.c:2586:3
    #2 0x58f94d in ssl3_read_bytes /root/heartbleed/BUILD/ssl/s3_pkt.c:1092:4
    #3 0x59418a in ssl3_get_message /root/heartbleed/BUILD/ssl/s3_both.c:457:7
    #4 0x55f3c7 in ssl3_get_client_hello /root/heartbleed/BUILD/ssl/s3_srvr.c:941:4
    #5 0x55b429 in ssl3_accept /root/heartbleed/BUILD/ssl/s3_srvr.c:357:9
    #6 0x51664d in LLVMFuzzerTestOneInput /root/FTS/openssl-1.0.1f/target.cc:34:3
[...]

0x629000009748 is located 0 bytes to the right of 17736-byte region [0x629000005200,
0x629000009748)
allocated by thread T0 here:
    #0 0x4e68e3 in __interceptor_malloc /tmp/final/llvm.src/projects/compiler-rt/lib/asan/asan_malloc_linux.cc:88:3
    #1 0x5c42cb in <strong><span>CRYPTO_malloc</span></strong> /root/heartbleed/BUILD/crypto/mem.c:308:8
    #2 0x5956c9 in freelist_extract /root/heartbleed/BUILD/ssl/s3_both.c:708:12
    #3 0x5956c9 in ssl3_setup_read_buffer /root/heartbleed/BUILD/ssl/s3_both.c:770
    #4 0x595cac in ssl3_setup_buffers /root/heartbleed/BUILD/ssl/s3_both.c:827:7
    #5 0x55bff4 in ssl3_accept /root/heartbleed/BUILD/ssl/s3_srvr.c:292:9
    #6 0x51664d in LLVMFuzzerTestOneInput /root/FTS/openssl-1.0.1f/target.cc:34:3
[...]</pre>
<p>The first portion of the output describes the type of issue (in this case, <code>heap-buffer-overflow</code>—specifically, a read access violation) and an easy-to-read symbolized stack trace pointing to the exact line of code that reads beyond the allocated buffer size. The second portion contains metadata about a nearby memory region and how it was allocated to help an engineer analyze the issue and understand how the process reached the invalid state.</p>
<p>The compiler and sanitizer instrumentation make this analysis possible. However, this instrumentation has limits: fuzzing with sanitizers doesn’t work as well when some portions of the software are handwritten assembly for performance reasons. The compiler can’t instrument the assembly code because the sanitizer plug-ins operate at a higher layer. As such, the handwritten assembly code that does not get instrumented may be responsible for false positives or undetected bugs.</p>
<p>Fuzzing entirely without sanitizers is possible, but it diminishes your ability to detect invalid program states and the metadata available to analyze a crash. For example, in order for fuzzing to produce any useful information if you’re not using a sanitizer, the program must encounter an “undefined behavior” scenario, and then signal this error state to the external fuzz engine (typically by crashing or exiting). Otherwise, the undefined behavior carries on undetected. Likewise, if you’re not using ASan or similar instrumentation, your fuzzer may not identify states where memory has been corrupted but is not used in a way that causes the operating system to terminate the process.</p>
<p>If you are working with libraries that are only available in binary form, compiler instrumentation is not an option. <a contenteditable="false" data-primary="AFL (American Fuzzy Lop)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="American Fuzzy Lop (AFL)" data-type="indexterm">&nbsp;</a>Some fuzz engines, like American Fuzzy Lop, also integrate with processor emulators like QEMU to instrument interesting instructions at the CPU level. This type of integration may be an appealing option for binary-only libraries you need to fuzz, at the expense of speed. This approach allows the fuzz engine to understand which code paths a generated input might prompt when compared to another generated input, but does not provide as much bug detection assistance as source code builds with compiler-added sanitizer instructions.</p>
<p><a contenteditable="false" data-primary="Honggfuzz" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="libFuzzer" data-type="indexterm">&nbsp;</a>Many modern fuzz engines, like <a href="https://llvm.org/docs/LibFuzzer.html">libFuzzer</a>, <a href="http://lcamtuf.coredump.cx/afl/">AFL</a>, and <a href="https://github.com/google/honggfuzz">Honggfuzz</a>, use some combination of the previously described techniques, or variations of these techniques. It’s possible to build a single fuzz driver that works with multiple fuzz engines. When working with multiple fuzz engines, it’s a good idea to make sure that you periodically move interesting input samples generated by each one back into the seed corpus that the other fuzz engines are configured to use. One engine might be successful at taking an input generated by another engine, mutating it, and causing a crash.<a contenteditable="false" data-primary="" data-startref="ch13.html10" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html9" data-type="indexterm">&nbsp;</a></p>
</section>
<section data-type="sect2" id="writing_effective_fuzz_drivers">
<h2>Writing Effective Fuzz Drivers</h2>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="writing effective fuzz drivers" data-type="indexterm">&nbsp;</a>To make these fuzzing concepts more concrete, we’ll go into more detail about a fuzz driver using the framework provided by LLVM’s libFuzzer engine, which is included with the Clang compiler. This particular framework is convenient because other fuzz engines (like Honggfuzz and AFL) also work with the libFuzzer entry point. As a fuzzer author, using this framework means you only have to write a single driver that implements the function prototype:</p>
<pre data-type="programlisting">int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size);</pre>
<p>The respective fuzz engines will then generate byte sequences and invoke your driver, which can pass the input to the code you want to test.</p>
<p>The goal of the fuzz engines is to execute the fuzz target via the driver as quickly as possible with as many unique and interesting inputs as they can generate. To enable reproducible crashes and quick fuzzing, try to avoid the following in your fuzz <span class="keep-together">drivers</span>:</p>
<ul>
<li><p>Nondeterministic behavior, such as relying on random number generators or specific multithreading behavior.</p></li>
<li><p>Slow operations, like console logging or disk I/O. Instead, consider creating “fuzzer-friendly” builds that disable these slow operations, or using a memory-based filesystem.</p></li>
<li><p>Crashing intentionally. The idea behind fuzzing is to find crashes you didn’t intend to have. The fuzz engine can’t disambiguate intentional crashes.</p></li>
</ul>
<p>These properties can also be desirable for the other types of testing described in this chapter.</p>
<p>You should also avoid any specialized integrity checks (like CRC32 or message digests) that an adversary can “fix up” in a generated input sample. The fuzz engine is unlikely to ever produce a valid checksum and pass the integrity check without specialized logic. A common convention is to use compiler preprocessor flags like <span class="keep-together"><code>-DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION</code></span> to enable this fuzzer-friendly behavior and to help reproduce crashes identified through fuzzing.</p>
</section>
<section data-type="sect2" id="an_example_fuzzer">
<h2>An Example Fuzzer</h2>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="example fuzzer" data-type="indexterm" id="ch13.html11">&nbsp;</a><a contenteditable="false" data-primary="Knusperli" data-type="indexterm" id="ch13.html12">&nbsp;</a>This section follows the steps of writing a fuzzer for a simple open source C++ library called <a href="https://github.com/google/knusperli">Knusperli</a>. Knusperli is a JPEG decoder that might see a wide range of input if it’s encoding user uploads or processing images (including potentially malicious images) from the web.</p>
<p>Knusperli also provides a convenient interface for us to fuzz: a function that accepts a sequence of bytes (the JPEG) and size parameter, as well as a parameter that controls which sections of the image to parse. For software that does not expose such a straightforward interface, you can use helper libraries like <a href="https://github.com/google/fuzzing/blob/master/docs/split-inputs.md#fuzzed-data-provider"><code>FuzzedDataProvider</code></a> to help transform the byte sequence into useful values for the target interface. Our example fuzz driver targets <a href="https://github.com/google/knusperli/blob/master/jpeg_data_reader.h#L42">this function</a>:</p>
<pre data-type="programlisting">bool ReadJpeg(const uint8_t* data, const size_t len, JpegReadMode mode, 
              JPEGData* jpg);</pre>
<p>Knusperli builds with the <a href="https://bazel.build">Bazel build system</a>. By modifying your <em>.bazelrc</em> file, you can create a convenient shorthand way to build targets using the various sanitizers, and build libFuzzer-based fuzzers directly. Here’s an example for ASan:</p>
<pre data-type="programlisting">$ <strong><span>cat ~/.bazelrc</span></strong>
build:asan --copt -fsanitize=address --copt -O1 --copt -g -c dbg
build:asan --linkopt -fsanitize=address --copt -O1 --copt -g -c dbg
build:asan --copt -fno-omit-frame-pointer --copt -O1 --copt -g -c dbg</pre>
<p>At this point, you should be able to build a version of the tool with ASan enabled:</p>
<pre data-type="programlisting">$ <strong><span>CC=clang-6.0 CXX=clang++-6.0 bazel build --config=asan :knusperli</span></strong></pre>
<p>You can also add a rule to the <em>BUILD</em> file for the fuzzer we’re about to write:</p>
<pre data-type="programlisting">cc_binary(
    name = "fuzzer",
    srcs = [
        "jpeg_decoder_fuzzer.cc",
    ],
    deps = [
        ":jpeg_data_decoder",
        ":jpeg_data_reader",
    ],
    linkopts = ["-fsanitize=address,fuzzer"],
)</pre>
<p><a data-type="xref" href="#example_onethree_twodot_jpeg_decoder_fu">example_onethree_twodot_jpeg_decoder_fu</a> shows what a simple attempt at the fuzz driver might look like.</p>
<div data-type="example" id="example_onethree_twodot_jpeg_decoder_fu">
<h5>jpeg_decoder_fuzzer.cc</h5>
<pre data-type="programlisting"> 1  #include &lt;cstddef&gt;
 2  #include &lt;cstdint&gt;
 3  #include "jpeg_data_decoder.h"
 4  #include "jpeg_data_reader.h"
 5  
 6  extern "C" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t sz) {
 7    knusperli::JPEGData jpg;
 8    knusperli::ReadJpeg(data, sz, knusperli::JPEG_READ_HEADER, &amp;jpg);
 9      return 0;
10  }</pre>
</div>
<p>We can build and run the fuzz driver with these commands:</p>
<pre data-type="programlisting">$ <strong><span>CC=clang-6.0 CXX=clang++-6.0 bazel build --config=asan :fuzzer</span></strong>
$ <strong><span>mkdir synthetic_corpus</span></strong>
$ <strong><span>ASAN_SYMBOLIZER_PATH=/usr/lib/llvm-6.0/bin/llvm-symbolizer bazel-bin/fuzzer \</span></strong>
<strong>  <span>-max_total_time 300 -print_final_stats synthetic_corpus/</span></strong></pre>
<p>The preceding command runs the fuzzer for five minutes using an empty input corpus. LibFuzzer places interesting generated samples in the <em>synthetic_corpus/</em> directory to use in future fuzzing sessions. You receive the following results:</p>
<pre data-type="programlisting">[...]
INFO:        0 files found in synthetic_corpus/
INFO: -max_len is not provided; libFuzzer will not generate inputs larger than 
4096 bytes
INFO: A corpus is not provided, starting from an empty corpus
#2      INITED cov: 110 ft: 111 corp: 1/1b exec/s: 0 rss: 36Mb
[...]
<strong><span>#3138182       DONE   cov: 151 ft: 418 corp: 30/4340b exec/s: 10425 rss: 463Mb</span></strong>
[...]
Done 3138182 runs in 301 second(s)
stat::number_of_executed_units: 3138182
stat::average_exec_per_sec:     10425
stat::new_units_added:          608
stat::slowest_unit_time_sec:    0
stat::peak_rss_mb:              463</pre>
<p>Adding a JPEG file—for example, the color bar pattern seen on broadcast TV—to the seed corpus also results in improvements. That single seed input brings &gt;10% improvement in the code blocks executed (the <code>cov</code> metric):</p>
<pre data-type="programlisting">#2      INITED cov: 169 ft: 170 corp: 1/8632b exec/s: 0 rss: 37Mb</pre>
<p>To reach even more code, we can use different values for the <code>JpegReadMode</code> parameter. The <a href="https://github.com/google/knusperli/blob/master/jpeg_data_reader.h#L31">valid values</a> are as follows:</p>
<pre data-type="programlisting">enum JpegReadMode {
  JPEG_READ_HEADER,   <em>// only basic headers</em>
  JPEG_READ_TABLES,   <em>// headers and tables (quant, Huffman, ...)</em>
  JPEG_READ_ALL,      <em>// everything</em>
};</pre>
<p>Rather than writing three different fuzzers, we can hash a subset of the input and use that result to exercise different combinations of library features in a single fuzzer. Be careful to use enough input to create a varied hash output. If the file format mandates that the first <em>N</em> bytes of an input all look the same, use at least one more than <em>N</em> when deciding what bytes will influence which options to set.</p>
<p>Other approaches include using the previously mentioned <code>FuzzedDataProvider</code> to split the input, or dedicating the first few bytes of input to setting the library parameters. The remaining bytes are then passed as the input to the fuzz target. Whereas hashing the input may result in wildly different configuration if a single input bit changes, the alternative approaches to splitting the input allow the fuzz engine to better track the relationship between the selected options and the way the code behaves. Be mindful of how these different approaches can affect the usability of potential existing seed inputs. In this case, imagine that you create a new pseudoformat by deciding to rely on the first few input bytes to set the options to the library. As a result, you can no longer easily use all the existing JPEG files in the world as possible seed inputs, unless you first preprocess the files to add initial parameters.</p>
<p>To explore the idea of configuring the library as a function of the generated input sample, we’ll use the number of bits set in the first 64 bytes of input to select a <span class="keep-together"><code>JpegReadMode</code></span>, as illustrated in <a data-type="xref" href="#example_onethree_threedot_fuzzing_by_sp">#example_onethree_threedot_fuzzing_by_sp</a>.</p>
<div data-type="example" id="example_onethree_threedot_fuzzing_by_sp">
<h5>Fuzzing by splitting the input</h5>
<pre data-type="programlisting">  #include &lt;cstddef&gt;
  #include &lt;cstdint&gt;
  #include "jpeg_data_decoder.h"
  #include "jpeg_data_reader.h"
  
  const unsigned int kInspectBytes = 64;
  const unsigned int kInspectBlocks = kInspectBytes / sizeof(unsigned int);
  
  extern "C" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t sz) {
   knusperli::JPEGData jpg;
   knusperli::JpegReadMode rm;
   unsigned int bits = 0;
 
   if (sz &lt;= kInspectBytes) {  <em>// Bail on too-small inputs.</em>
     return 0;
   }
 
   for (unsigned int block = 0; block &lt; kInspectBlocks; block++) {
     bits += 
       __builtin_popcount(reinterpret_cast&lt;const unsigned int *&gt;(data)[block]);
   }
 
   rm = static_cast&lt;knusperli::JpegReadMode&gt;(bits % 
                                             (knusperli::JPEG_READ_ALL + 1));
 
   knusperli::ReadJpeg(data, sz, rm, &amp;jpg);

   return 0;
 }</pre>
</div>
<p>When using the color bar as the only input corpus for five minutes, this fuzzer gives the following results:</p>
<pre data-type="programlisting"><strong><span>#851071 DONE   cov: 196 ft: 559 corp: 51/29Kb exec/s: 2827 rss: 812Mb</span></strong>
[...]
Done 851071 runs in 301 second(s)
stat::number_of_executed_units: 851071
stat::average_exec_per_sec:     2827
stat::new_units_added:          1120
stat::slowest_unit_time_sec:    0
stat::peak_rss_mb:              812</pre>
<p>Executions per second have dropped because the changes enable more features of the library, causing this fuzz driver to reach much more code (indicated by the rising <code>cov</code> metric). If you run the fuzzer without any time-out limits, it will continue to generate inputs indefinitely until the code provokes a sanitizer error condition. At that point, you will see a report like the one shown earlier for the Heartbleed bug. You can then make code changes, rebuild, and run the fuzzer binary that you built with the saved artifact as a way to reproduce the crash or to verify that the code change will fix the issue.<a contenteditable="false" data-primary="" data-startref="ch13.html12" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html11" data-type="indexterm">&nbsp;</a></p>
</section>
<section data-type="sect2" id="continuous_fuzzing">
<h2>Continuous Fuzzing</h2>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="continuous fuzzing" data-type="indexterm">&nbsp;</a>Once you have written some fuzzers, running them regularly over a codebase as it’s developed can provide a valuable feedback loop to engineers. A continuous build pipeline can generate daily builds of fuzzers in your codebase to be consumed by a system that runs the fuzzers, collects crash information, and files bugs in an issue tracker. Engineering teams can use the results to focus on identifying vulnerabilities or eliminating root causes that make the service miss its SLO.</p>
<section data-type="sect3" id="example_clusterfuzz_and_ossfuzz">
<h3>Example: ClusterFuzz and OSSFuzz</h3>
<p><a contenteditable="false" data-primary="ClusterFuzz" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="ClusterFuzz" data-type="indexterm">&nbsp;</a><a href="https://github.com/google/clusterfuzz">ClusterFuzz</a> is an open source implementation of a scalable fuzzing infrastructure released by Google. It manages pools of virtual machines that run fuzzing tasks and provides a web interface to view information about the fuzzers. ClusterFuzz does not build fuzzers, but instead expects a continuous build/integration pipeline to push <span class="keep-together">fuzzers</span> to a Google Cloud Storage bucket. It also provides services like corpus management, crash deduplication, and lifecycle management for the crashes that it identifies. The heuristics ClusterFuzz uses for crash deduplication are based on the state of the program at the time of the crash. By preserving the samples that cause a crash, ClusterFuzz can also periodically retest these issues to determine whether they still reproduce, and automatically close the issue when the latest version of the fuzzer no longer crashes on the offending sample.</p>
<p>The ClusterFuzz web interface shows metrics you can use to understand how well a given fuzzer is performing. The metrics available depend on what’s exported by the fuzz engines integrated into your build pipeline (as of early 2020, ClusterFuzz supports libFuzzer and AFL). The ClusterFuzz documentation provides instructions for extracting code coverage information from fuzzers built with Clang code coverage support, then converting that information into a format you can store in a Google Cloud Storage bucket and display in the frontend. Using this functionality to explore the code covered by the fuzzer written in the previous section would be a good next step for determining additional improvements to the input corpus or fuzz driver.</p>
<p><a contenteditable="false" data-primary="fuzz testing (fuzzing)" data-secondary="OSS-Fuzz" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="OSS-Fuzz" data-type="indexterm">&nbsp;</a><a href="https://github.com/google/oss-fuzz/">OSS-Fuzz</a> combines <a href="https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html">modern fuzzing techniques</a> with a scalable distributed execution of ClusterFuzz that’s hosted on the Google Cloud Platform. It uncovers security vulnerabilities and stability issues, and reports them directly to developers—within five months of its launch in December 2016, OSS-Fuzz had discovered <a href="https://testing.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html">over a thousand bugs</a>, and since then it has found tens of thousands more.</p>
<p>Once a project is <a href="https://google.github.io/oss-fuzz/getting-started/new-project-guide/">integrated</a> with OSS-Fuzz, the tool uses continuous and automated testing to find issues only hours after modified code is introduced into the upstream repository, before any users are affected. At Google, by unifying and automating our fuzzing tools, we’ve consolidated our processes into a single workflow based on OSS-Fuzz. These integrated OSS projects also benefit from being <a href="https://security.googleblog.com/2018/11/a-new-chapter-for-oss-fuzz.html">reviewed</a> by both Google’s internal tools and external fuzzing tools. Our integrated approach increases code coverage and discovers bugs faster, improving the security posture of Google projects and the open source ecosystem.<a contenteditable="false" data-primary="" data-startref="ch13.html8" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html7" data-type="indexterm">&nbsp;</a></p>
</section>
</section>
</section>
<section data-type="sect1" id="static_program_analysis">
<h1 class="dive">Static Program Analysis</h1>
<p><a contenteditable="false" data-primary="program analysis" data-secondary="static" data-seealso="static analysis" data-type="indexterm" id="ch13.html13">&nbsp;</a><a contenteditable="false" data-primary="static analysis" data-type="indexterm" id="ch13.html14">&nbsp;</a><a contenteditable="false" data-primary="testing (code)" data-secondary="static program analysis" data-type="indexterm" id="ch13.html15">&nbsp;</a><em>Static analysis</em> is a means of analyzing and understanding computer programs by inspecting their source code without executing or running them. Static analyzers parse the source code and build an internal representation of the program that’s suitable for automated analysis. This approach can discover potential bugs in source code, preferably before the code is checked in or deployed in <span class="keep-together">production</span>. Numerous <a href="https://en.wikipedia.org/wiki/List_of_tools_for_static_code_analysis">tools</a> are available for various languages, as well as tools for cross-language analyses.</p>
<p>Static analysis tools make different tradeoffs between depth of analysis versus cost of analyzing the source code. For example, the shallowest analyzers perform simple textual or abstract syntax tree (AST)–based pattern matches. Other techniques rely on reasoning about a program’s state-based semantic constructs, and basing that reasoning on the program’s control flow and data flow.</p>
<p><a contenteditable="false" data-primary="false positives/negatives" data-type="indexterm">&nbsp;</a>Tools also target different analysis tradeoffs between false positives (incorrect warnings) and false negatives (missed warnings). Tradeoffs are unavoidable, partially because of the fundamental limit to static analysis: statically verifying any program is an <a href="https://en.wikipedia.org/wiki/Undecidable_problem"><em>undecidable problem</em></a>—that is, it is not possible to develop an algorithm that can determine whether any given program will execute without violating any given <span class="keep-together">property</span>.</p>
<p>Given that constraint, tool providers focus on generating useful signals for developers at various stages of development. Depending on the integration point for static analysis engines, different tradeoffs with respect to analysis speed and expected analysis feedback are acceptable. For example, a static analysis tool that’s integrated within a code review system will likely target only newly developed source code, and will issue precise warnings that focus on very probable issues. On the other hand, source code undergoing a final predeployment release analysis for a safety-critical program (e.g., for domains like avionics software or medical device software with potential governmental certification requirements) may require more formal and stricter analysis.<sup><a data-type="noteref" id="ch13fn6-marker" href="#ch13fn6">6</a></sup></p>
<p>The following sections present static analysis techniques adapted for various needs during different stages of the development process. We highlight automated code inspection tools, abstract interpretation–based tools (this process is sometimes referred to as <em>deep static analysis</em>), and more resource-intensive approaches, such as formal methods. We also discuss how to integrate static analyzers into developer workflows.</p>
<section data-type="sect2" id="automated_code_inspection_tools">
<h2>Automated Code Inspection Tools</h2>
<p><a contenteditable="false" data-primary="automated code inspection tools" data-type="indexterm" id="ch13.html16">&nbsp;</a><a contenteditable="false" data-primary="code inspection tools, automated" data-type="indexterm" id="ch13.html17">&nbsp;</a><a contenteditable="false" data-primary="static analysis" data-secondary="automated code inspection tools" data-type="indexterm" id="ch13.html18">&nbsp;</a><em>Automated code inspection tools</em> perform a syntactic analysis of source code with respect to language features and usage rules. <a contenteditable="false" data-primary="linters" data-type="indexterm">&nbsp;</a>These tools, commonly referred to as <em>linters</em>, generally don’t model complex program behaviors like interprocedural data flow. Because they perform relatively shallow analysis, the tools scale easily to arbitrary code sizes—they can often complete their source code analysis in about the same amount of time it takes to compile the code. Code inspection tools are also easily extensible—you can simply add new rules that cover many types of bugs, especially bugs related to language features.</p>
<p>Over the past few years, code inspection tools have focused on stylistic and readability changes, because such code improvement suggestions have a high acceptance rate by developers. Many organizations enforce style and format checks by default in order to maintain a cohesive codebase that’s easier to manage across large developer teams. These organizations also routinely run checks that reveal potential <a href="https://en.wikipedia.org/wiki/Code_smell">code smells</a> and highly likely bugs.</p>
<p><a contenteditable="false" data-primary="AST pattern matching" data-type="indexterm" id="ch13.html19">&nbsp;</a>The following example focuses on tools that perform one particular type of analysis—AST pattern matching. An <em>AST</em> is a tree representation of a program’s source code based on the syntactic structure of the programming language. Compilers commonly parse a given source code input file into such a representation, and then manipulate that representation during the compilation process. For example, an AST may contain a node representing an <code>if-then-else</code> construct, which has three child nodes: one node for the condition of the <code>if</code> statement, one node representing the subtree for the <code>then</code> branch, and another node representing the subtree of the <code>else</code> branch.</p>
<p><a contenteditable="false" data-primary="Error Prone" data-type="indexterm">&nbsp;</a><a href="https://errorprone.info">Error Prone</a> for Java <a contenteditable="false" data-primary="Clang-Tidy" data-type="indexterm" id="ch13.html20">&nbsp;</a>and <a href="https://clang.llvm.org/extra/clang-tidy.html">Clang-Tidy</a> for C/C++ are widely used across projects at Google. Both of these analyzers allow engineers to add custom checks. For example, <a href="https://cacm.acm.org/magazines/2018/4/226371-lessons-from-building-static-analysis-tools-at-google/fulltext">as of early 2018</a>, 162 authors had submitted 733 checks to Error Prone. For certain types of bugs, both Error Prone and Clang-Tidy can produce suggested fixes. Some compilers (like Clang and MSVC) also support the community-developed <a href="https://github.com/isocpp/CppCoreGuidelines">C++ core guidelines</a>. With help from the Guideline Support Library (GSL), these guidelines prevent many common mistakes in C++ programs.</p>
<p>AST pattern-matching tools allow users to add new checks by writing rules on the parsed AST. For example, consider the <a href="http://clang.llvm.org/extra/clang-tidy/checks/abseil-string-find-startswith.html"><code>absl-string-find-startsWith</code></a> Clang-Tidy warning. The tool attempts to improve the readability and performance of code that checks for string prefix matches using the C++ <a href="http://www.cplusplus.com/reference/string/string/find/"><code>string::find</code> API</a>: Clang-Tidy recommends using the <code>StartsWith</code> API provided by <a href="https://clang.llvm.org/extra/clang-tidy/checks/abseil-string-find-startswith.html">ABSL</a> instead. To perform its analysis, the tool creates an AST subtree pattern that compares the output of the C++ <code>string::find</code> API with the integer value 0. The Clang-Tidy infrastructure provides the tooling to find AST subtree patterns in the AST representation of the program being analyzed.</p>
<p>Consider the following code snippet:</p>
<pre data-type="programlisting">std::string s = "...";
if (s.find("Hello World") == 0) { /* do something */ }</pre>
<p>The <code>absl-string-find-startsWith</code> Clang-Tidy warning flags this code snippet and suggests that the code be changed in the following way:</p>
<pre data-type="programlisting">std::string s = "...";
if (absl::StartsWith(s, "Hello World")) { /* do something */ }</pre>
<p>In order to suggest a fix, Clang-Tidy (conceptually speaking) provides the capability to transform an AST subtree, given a pattern. The left side of <a data-type="xref" href="#ast_pattern_match_and_replacement_sugge">Figure 13-1</a> shows an AST pattern match. (The AST subtree is simplified for the sake of clarity.) If the tool finds a matching AST subtree in the parse AST tree of the source code, it notifies the developer. AST nodes also contain line and column information, which allows the AST pattern-matcher to report a specific warning to the developer.</p>
<figure id="ast_pattern_match_and_replacement_sugge">
<img src="images/bsrs_1301.png" alt="Figure 13-1: AST pattern-match and replacement suggestion"/>
<figcaption>Figure 13-1: AST pattern-match and replacement suggestion</figcaption>
</figure>
<p>In addition to performance and readability checks, Clang-Tidy also provides many common bug pattern checks. Consider running Clang-Tidy on the following input files:<sup><a data-type="noteref" id="ch13fn7-marker" href="#ch13fn7">7</a></sup></p>
<pre data-type="programlisting">$ <strong><span>cat -n sizeof.c</span></strong>
 1  #include &lt;string.h&gt;
 2  const char* kMessage = "Hello World!";
 3  int main() {
 4    char buf[128];
 5    memcpy(buf, kMessage, sizeof(kMessage));
 6    return 0;
 7  }

$ <strong><span>clang-tidy sizeof.c</span></strong>
[...]
Running without flags.
<strong><span>1 warning generated.</span></strong>
<strong><span>sizeof.c:5</span></strong>:32: warning: 'memcpy' call operates on objects of type 'const char' 
while the size is based on a different type 'const char *' 
<strong><span>[clang-diagnostic-sizeof-pointer-memaccess]</span></strong>
  memcpy(buf, kMessage, sizeof(kMessage));
                               ^
<strong><span>sizeof.c:5</span></strong>:32: note: did you mean to provide an explicit length?
  memcpy(buf, kMessage, sizeof(kMessage));

$ <strong><span>cat -n sizeof2.c</span></strong>
 1  #include &lt;string.h&gt;
 2  const char kMessage[] = "Hello World!";
 3  int main() {
 4    char buf[128];
 5    memcpy(buf, kMessage, sizeof(kMessage));
 6    return 0;
 7  }

$ <strong><span>clang-tidy sizeof2.c</span></strong>
[...]
Running without flags.</pre>
<p>The two input files differ only in the type declaration of <code>kMessage</code>. When <code>kMessage</code> is defined as a pointer to initialized memory, <code>sizeof(kMessage)</code> returns the size of a pointer type. Therefore, Clang-Tidy produces the <a href="http://clang.llvm.org/extra/clang-tidy/checks/bugprone-sizeof-expression.html"><code>clang-diagnostic-sizeof-pointer-memaccess</code></a> warning. On the other hand, when <code>kMessage</code> is of type <code>const char[]</code>, the <code>sizeof(kMessage)</code> operation returns the appropriate, expected length, and Clang-Tidy doesn’t produce a warning.</p>
<p>For some pattern checks, in addition to reporting a warning, Clang-Tidy can suggest code fixes. The <code>absl-string-find-startsWith</code> Clang-Tidy warning suggestion presented earlier is one such instance. The righthand side of <a data-type="xref" href="#ast_pattern_match_and_replacement_sugge">Figure 13-1</a> shows the appropriate AST-level replacement. When such suggestions are available, you can tell Clang-Tidy to automatically apply them to the input file, using the <code>--fix</code> command-line option.<a contenteditable="false" data-primary="" data-startref="ch13.html20" data-type="indexterm">&nbsp;</a></p>
<p>You can also use automatically applied suggestions to update a codebase using the Clang-Tidy <code>modernize</code> fixes. Consider the following sequence of commands, which showcases the <a href="http://clang.llvm.org/extra/clang-tidy/checks/modernize-use-nullptr.html"><code>modernize-use-nullptr</code></a> pattern. The sequence finds instances of zero-constants used for pointer assignments or comparisons, and changes them to use <code>nullptr</code> instead. In order to run all <code>modernize</code> checks, we use Clang-Tidy with the option <code>--checks=modernize-*</code>; then <code>--fix</code> applies the suggestions to the input file. At the end of the sequence of commands, we highlight the four changes by printing the transformed file (emphasis added):</p>
<pre data-type="programlisting">$ <strong><span>cat -n nullptr.cc</span></strong>
 1  #define NULL 0x0
 2
 3  int *ret_ptr() {
 4    return 0;
 5  }
 6
 7  int main() {
 8    char *a = NULL;
 9    char *b = 0;
10    char c = 0;
11    int *d = ret_ptr();
12    return d == NULL ? 0 : 1;
13  }

$ <strong><span>clang-tidy nullptr.cc -checks=modernize-* --fix</span></strong>
[...]
Running without flags.
<strong><span>4 warnings generated.</span></strong>
nullptr.cc:4:10: warning: use nullptr [modernize-use-nullptr]
  return 0;
         ^
         nullptr
nullptr.cc:4:10: note: FIX-IT applied suggested code changes
  return 0;
         ^
nullptr.cc:8:13: warning: use nullptr [modernize-use-nullptr]
  char *a = NULL;
            ^
            nullptr
nullptr.cc:8:13: note: FIX-IT applied suggested code changes
  char *a = NULL;
            ^
nullptr.cc:9:13: warning: use nullptr [modernize-use-nullptr]
  char *b = 0;
            ^
            nullptr
nullptr.cc:9:13: note: FIX-IT applied suggested code changes
  char *b = 0;
            ^
nullptr.cc:12:15: warning: use nullptr [modernize-use-nullptr]
  return d == NULL ? 0 : 1;
              ^
              nullptr
nullptr.cc:12:15: note: FIX-IT applied suggested code changes
  return d == NULL ? 0 : 1;
              ^
<strong><span>clang-tidy applied 4 of 4 suggested fixes.</span></strong>

$ <strong><span>cat -n nullptr.cc</span></strong>
 1  #define NULL 0x0
 2
 3  int *ret_ptr() {
 4    return <strong><span>nullptr</span></strong>;
 5  }
 6
 7  int main() {
 8    char *a = <strong><span>nullptr</span></strong>;
 9    char *b = <strong><span>nullptr</span></strong>;
10    char c = 0;
11    int *d = ret_ptr();
12    return d == <strong><span>nullptr</span></strong> ? 0 : 1;
13  }</pre>
<p>Other languages have similar automated code inspection tools.<a contenteditable="false" data-primary="" data-startref="ch13.html19" data-type="indexterm">&nbsp;</a> For example, <a href="https://golang.org/cmd/vet/">GoVet</a> analyzes Go source code for common suspicious constructs, <a href="https://www.pylint.org">Pylint</a> analyzes Python code, <a contenteditable="false" data-primary="Error Prone" data-type="indexterm">&nbsp;</a>and Error Prone provides analysis and auto-fix capabilities for Java programs. The following example briefly demonstrates running Error Prone via Bazel build rule (emphasis added). In Java, the subtraction operation <code>i-1</code> on the variable <code>i</code> of type <code>Short</code> returns a value of type <code>int</code>. Therefore, it is infeasible for the <code>remove</code> operation to succeed:<a contenteditable="false" data-primary="" data-startref="ch13.html18" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html17" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html16" data-type="indexterm">&nbsp;</a></p>
<pre data-type="programlisting">$ <strong><span>cat -n ShortSet.java</span></strong>
 1  import java.util.Set;
 2  import java.util.HashSet;
 3
 4  public class ShortSet {
 5    public static void main (String[] args) {
 6      Set&lt;Short&gt; s = new HashSet&lt;&gt;();
 7      for (short i = 0; i &lt; 100; i++) {
 8        s.add(i);
 9        s.remove(i - 1);
10      }
11      System.out.println(s.size());
12    }
13  }

$ <strong><span>bazel build :hello</span></strong>
ERROR: example/myproject/BUILD:29:1: Java compilation in rule '//example/myproject:hello'
<strong><span>ShortSet.java:9: error: [CollectionIncompatibleType]</span></strong> Argument 'i - 1' should not be 
passed to this method;
its type int is not compatible with its collection's type argument Short
      s.remove(i - 1);
              ^
    (see http://errorprone.info/bugpattern/CollectionIncompatibleType)
1 error</pre>
</section>
<section data-type="sect2" id="integration_of_static_analysis_in_the_d">
<h2>Integration of Static Analysis in the Developer Workflow</h2>
<p><a contenteditable="false" data-primary="static analysis" data-secondary="integration into developer workflow" data-type="indexterm" id="ch13.html21">&nbsp;</a>It’s considered good industry practice to run relatively fast static analysis tools as early as possible in the development cycle. Finding bugs early is important because the cost of fixing them increases substantially if they’re pushed into the source code repository or deployed to users.</p>
<p class="pagebreak-before">There’s a low barrier to integrating static analysis tools into your CI/CD pipeline, with a potentially high positive impact on the productivity of your engineers. For example, a developer can get an error and a suggestion about how to fix null pointer dereferencing. And if they can’t push their code, they can’t forget to fix the issue and accidentally cause the system to crash or expose information, which contributes to a culture of security and reliability (see <a data-type="xref" href="#twoone_building_a_culture_of_security_a">Chapter 21</a>).</p>
<p><a contenteditable="false" data-primary="Shipshape" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Tricorder" data-type="indexterm">&nbsp;</a>To this end, Google developed the Tricorder program analysis platform<sup><a data-type="noteref" id="ch13fn8-marker" href="#ch13fn8">8</a></sup> and an open-source version of Tricorder called <a href="https://github.com/google/shipshape">Shipshape</a>. Tricorder performs static analysis of approximately 50,000 code review changes per day. The platform runs many types of program analysis tools and surfaces warnings to developers during code review, when they are accustomed to evaluating suggestions. The tools aim to provide code findings that are easy to understand and easy to fix, with a low user-perceived false-positive rate (10%, at most).</p>
<p>Tricorder is designed to allow users to run many different program analysis tools. As of early 2018, the platform included 146 analyzers covering over 30 source languages. Most of these analyzers were contributed by Google developers. Generally speaking, commonly available static analysis tools are not very complex. Most checkers run by Tricorder are automated code inspection tools. These tools target a variety of languages, check for conformance to coding style guidelines, and find bugs. As previously mentioned, Error Prone and Clang-Tidy can produce suggested fixes in certain scenarios. The code author can then apply the fixes with the click of a button.</p>
<p><a data-type="xref" href="#screenshot_of_static_analysis_results_d">Figure 13-2</a> shows a screenshot of Tricorder analysis results for a given Java input file, as presented to a code reviewer. The results show two warnings, one from the Java linter and one from Error Prone. Tricorder measures the user-perceived false-positive rate by allowing code reviewers to provide feedback on surfaced warnings via a “Not useful” link. The Tricorder team uses these signals to disable individual checks. The code reviewer can also send a request to the code author to “Please fix” an individual warning.</p>
<figure id="screenshot_of_static_analysis_results_d">
<img src="images/bsrs_1302.png" alt="Figure 13-2: Screenshot of static analysis results during code review provided via Tricorder"/>
<figcaption>Figure 13-2: Screenshot of static analysis results during code review provided via <span class="keep-together">Tricorder</span></figcaption>
</figure>
<p><a data-type="xref" href="#screenshot_of_the_preview_fix_view_for">Figure 13-3</a> shows the automatically applied code changes suggested by Error Prone during code review.</p>
<figure id="screenshot_of_the_preview_fix_view_for">
<img src="images/bsrs_1303.png" alt="Figure 13-3: Screenshot of the preview fix view for the Error Prone warning from"/>
<figcaption>Figure 13-3: Screenshot of the preview fix view for the Error Prone warning from <a data-type="xref" href="#screenshot_of_static_analysis_results_d">Figure 13-2</a></figcaption>
</figure>
<aside data-type="sidebar" id="reverse_engineering_and_test_input_gene">
<h5>Reverse Engineering and Test Input Generation</h5>
<p><a contenteditable="false" data-primary="reverse engineering" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="static analysis" data-secondary="reverse engineering and test input generation" data-type="indexterm">&nbsp;</a>Program analysis techniques, both static and dynamic, have been used for purposes beyond testing and ensuring code correctness—for example, to reverse engineer software. Reverse engineering can be useful when trying to understand the behavior of binaries where source code is not available. A common use case for reverse engineering occurs when a security engineer is trying to understand a potentially malicious binary. <a contenteditable="false" data-primary="decompilers" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="disassemblers" data-type="indexterm">&nbsp;</a>This analysis often involves using so-called <em>decompilers</em> or <em>disassemblers:</em> a disassembler translates machine language into assembly code, while a decompiler translates from machine language to source code. These tools do not guarantee that they can re-create the original source code itself. However, the generated code can help a security engineer who is trying to understand the program and its behavior. One popular tool for reverse engineering is <a href="https://ghidra-sre.org">Ghidra</a>.</p>
<p>Some engineers also use program analysis techniques to help with test input generation. <a contenteditable="false" data-primary="concolic testing" data-type="indexterm">&nbsp;</a>One technique that has recently become popular, <em>concolic testing,</em> combines a regular execution (called a <em>concrete</em> execution since it utilizes concrete values) with symbolic analysis techniques (hence the name concolic—<em>conc</em>rete + symb<em>olic</em>). Users can then automatically generate test inputs that are guaranteed to cover some other execution paths in the target program. Concolic tests can make this guarantee by executing a given program given a concrete input (e.g., the integer value <code>123</code>), while shadowing each execution step with a formula that corresponds to the observed statements and branches. For example, you might substitute the concrete input value <code>123</code> with the symbol α. As the execution proceeds, at every branch point, such as an <code>if</code> statement, concolic execution asks a constraint solver whether it is possible to find a different value for α that would lead down the alternative branch. With each collected input value, you can start new concolic executions that increase branch coverage. <a href="https://klee.github.io">KLEE</a> is one popular concolic testing tool.<a contenteditable="false" data-primary="" data-startref="ch13.html21" data-type="indexterm">&nbsp;</a></p>
</aside>
</section>
<section data-type="sect2" id="abstract_interpretation">
<h2>Abstract Interpretation</h2>
<p><a contenteditable="false" data-primary="abstract interpretation" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="static analysis" data-secondary="abstract interpretation" data-type="indexterm">&nbsp;</a>Abstract interpretation–based tools statically perform a semantic analysis of program behaviors.<sup><a data-type="noteref" id="ch13fn9-marker" href="#ch13fn9">9</a></sup> This technique has been used successfully to verify safety-critical software, such as flight control software.<sup><a data-type="noteref" id="ch13fn10-marker" href="#ch13fn10">10</a></sup> Consider the simple example of a program that generates the 10 smallest positive even integers. During its regular execution, the program generates the integer values 2, 4, 6, 8, 10, 12, 14, 16, 18, and 20. In order to allow efficient static analysis of such a program, we want to summarize all the possible values using a compact representation that covers all the observed values. Using the so-called interval or range domain, we can represent all observed values using the abstract interval value [2, 20] instead. The interval domain allows the static analyzer to reason efficiently about all program executions by simply remembering the lowest and highest possible values.</p>
<p>In order to ensure that we’re capturing all possible program behaviors, it’s important to cover all observed values with the abstract representation. However, this approach also introduces an approximation that may lead to <em>imprecision</em>, or false warnings. For example, if we wanted to guarantee that the actual program never produces the value 11, an analysis using an integer domain would lead to a false positive.</p>
<p>Static analyzers utilizing abstract interpretation generally compute an abstract value for every program point. <a contenteditable="false" data-primary="CFG (control-flow graph)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="control-flow graph (CFG)" data-type="indexterm">&nbsp;</a>To do so, they rely on a <em>control-flow graph</em> (CFG) representation of a program. CFGs are commonly used during compiler optimizations, and to statically analyze programs. Each node in a CFG represents a basic block in the program, which corresponds to a sequence of program statements that are always executed in order. That is, there are no jumps from within this sequence of statements, and no jump targets in the middle of the sequence. Edges in a CFG represent control flow in the program, where a jump occurs either through intraprocedural control flow—for example, due to <code>if</code> statements or loop constructs—or interprocedural control flow due to function calls. Note that the CFG representation is also used by coverage-guided fuzzers (discussed previously). For instance, libFuzzer keeps track of which basic blocks and edges are covered during fuzzing. The fuzzer uses this information to decide whether to consider an input for future mutations.</p>
<p>Abstract interpretation–based tools perform a semantic analysis that reasons about data flow and control flow in programs, often across function calls. For that reason, they take much longer to run than the previously discussed automated code inspection tools. While you can integrate automated code inspection tools into interactive development environments such as code editors, abstract interpretation is generally not similarly integrated. Instead, developers might run abstract interpretation–based tools on committed code occasionally (nightly, for example), or during code review in differential settings, analyzing only the changed code while reusing analysis facts for unchanged code.</p>
<p>A number of tools rely on abstract interpretation for a variety of languages and properties. <a contenteditable="false" data-primary="Frama-C" data-type="indexterm">&nbsp;</a>For example, the <a href="https://frama-c.com">Frama-C tool</a> allows you to find common runtime errors and assertion violations including buffer overflows, segmentation faults due to dangling or null pointers, and division by zero in programs written in C. As previously discussed, these types of bugs—especially memory-related bugs—can have security implications. <a contenteditable="false" data-primary="Infer" data-type="indexterm">&nbsp;</a>The <a href="https://fbinfer.com">Infer tool</a> reasons about memory and pointer changes performed by programs and can find bugs like dangling pointers in Java, C, and other languages. <a contenteditable="false" data-primary="AbsInt" data-type="indexterm">&nbsp;</a>The <a href="https://www.absint.com">AbsInt tool</a> can perform worst-case execution time analysis of tasks in real-time systems. <a contenteditable="false" data-primary="App Security Improvement (ASI)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="ASI (App Security Improvement)" data-type="indexterm">&nbsp;</a>The <a href="https://developer.android.com/google/play/asi">App Security Improvement (ASI) program</a> performs a sophisticated interprocedural analysis on every Android app that’s uploaded to the Google Play Store, for safety and security. If it finds a vulnerability, ASI flags the vulnerability and presents suggestions for addressing the issue. <a data-type="xref" href="#an_app_security_improvement_alert">Figure 13-4</a> shows a sample security alert. As of early 2019, this program had led to over <a href="https://android-developers.googleblog.com/2019/02/android-security-improvement-update.html?linkId=64187521">one million app fixes</a> in the Play Store by over 300,000 app developers.</p>
<figure id="an_app_security_improvement_alert">
<img src="images/bsrs_1304.png" alt="Figure 13-4: An App Security Improvement alert"/>
<figcaption>Figure 13-4: An App Security Improvement alert</figcaption>
</figure>
</section>
<section data-type="sect2" id="formal_methods">
<h2>Formal Methods</h2>
<p><a contenteditable="false" data-primary="static analysis" data-secondary="formal methods" data-type="indexterm">&nbsp;</a><em>Formal methods</em> allow users to specify properties of interest for software or hardware systems. Most of these are so-called <em>safety properties</em> that specify that a certain bad behavior should never be observable. For example, “bad behavior” can include assertions in programs. Others include <em>liveness properties,</em> which allow users to specify a desired outcome—for example, that a submitted print job is eventually processed by a printer. Users of formal methods can verify these properties for particular systems or models, and even develop such systems using <em>correct-by-construction</em>–based approaches. As highlighted in <a data-type="xref" href='ch06.html#analyzing_invariants'>Analyzing Invariants</a>, formal methods–based approaches often have a relatively high up-front cost. This is partially because these approaches require an a priori description of system requirements and properties of interest. These requirements must be specified in a mathematically rigorous and formal way.</p>
<p>Formal methods–based techniques have been successfully integrated into hardware design and verification tools.<sup><a data-type="noteref" id="ch13fn11-marker" href="#ch13fn11">11</a></sup> In hardware design, it is now standard practice to use formal or semiformal tools provided by electronic design automation (EDA) vendors. These techniques have also been successfully applied to software in specialized domains, such as safety-critical systems or cryptographic protocol analysis. For example, a formal methods–based approach continuously analyzes the cryptographic protocols used in TLS within computer network communications.<a contenteditable="false" data-primary="" data-startref="ch13.html15" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html14" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="" data-startref="ch13.html13" data-type="indexterm">&nbsp;</a><sup><a data-type="noteref" id="ch13fn12-marker" href="#ch13fn12">12</a></sup></p>
</section>
</section>
<section data-type="sect1" class="pagebreak-before" id="conclusion-id00012">
<h1 class="less_space">Conclusion</h1>
<p>Testing software for security and reliability is a vast topic, of which we’ve just scratched the surface. The testing strategies presented in this chapter, combined with practices around writing secure code to eliminate entire bug classes (see <a data-type="xref" href="#writing_code">Chapter 12</a>), have been key in helping Google teams scale reliably, minimizing outages and security problems. It’s important to build software with testability in mind from the earliest stages of development, and to engage in comprehensive testing throughout the development lifecycle.</p>
<p>At this point, we want to emphasize the value of fully integrating all of these testing and analysis methods into your engineering workflows and CI/CD pipelines. By combining and regularly using these techniques consistently throughout your codebase, you can identify bugs more quickly. You’ll also raise confidence in your ability to detect or prevent bugs when you deploy your applications—a topic covered in the next chapter.<a contenteditable="false" data-primary="" data-startref="ch13.html0" data-type="indexterm">&nbsp;</a></p>
</section>
</section>
</body>
</html>
<div data-type="footnotes">
<p data-type="footnote" id="ch13fn1"><sup><a href="#ch13fn1-marker">1</a></sup>We recommend checking out <a class="orm:hideurl" href="https://landing.google.com/sre/sre-book/chapters/testing-reliability/">Chapter 17 of the SRE book</a> for a reliability-focused perspective.</p>
<p data-type="footnote" id="ch13fn2"><sup><a href="#ch13fn2-marker">2</a></sup>Petrović, Goran, and Marko Ivanković. 2018. “State of Mutation Testing at Google.” <em>Proceedings of the 40th International Conference on Software Engineering</em>: 163–171. doi:10.1145/3183519.3183521.</p>
<p data-type="footnote" id="ch13fn3"><sup><a href="#ch13fn3-marker">3</a></sup>For more discussion of common unit testing pitfalls encountered at Google, see Wright, Hyrum, and Titus Winters. 2015. “All Your Tests Are Terrible: Tales from the Trenches.” CppCon 2015. <a href="https://www.youtube.com/watch?v=u5senBJUkPc"><em class="hyperlink">https://www.youtube.com/watch?v=u5senBJUkPc</em></a>.<a contenteditable="false" data-primary="" data-startref="ch13.html3" data-type="indexterm">&nbsp;</a></p>
<p data-type="footnote" id="ch13fn4"><sup><a href="#ch13fn4-marker">4</a></sup>See <a class="orm:hideurl" href="https://landing.google.com/sre/workbook/chapters/implementing-slos/">Chapter 2 of the SRE workbook</a>.</p>
<p data-type="footnote" id="ch13fn5"><sup><a href="#ch13fn5-marker">5</a></sup>The fuzz target compares the results of two modular exponentiation implementations inside OpenSSL, and will fail if the results ever differ.</p>
<p data-type="footnote" id="ch13fn6"><sup><a href="#ch13fn6-marker">6</a></sup>For an example, see Bozzano, Marco et al. 2017. “Formal Methods for Aerospace Systems.” In <em>Cyber-Physical System Design from an Architecture Analysis Viewpoint</em>, edited by Shin Nakajima, Jean-Pierre Talpin, Masumi Toyoshima, and Huafeng Yu. Singapore: Springer.</p>
<p data-type="footnote" id="ch13fn7"><sup><a href="#ch13fn7-marker">7</a></sup>You can install Clang-Tidy using standard package managers. It is generally called clang-tidy.</p>
<p data-type="footnote" id="ch13fn8"><sup><a href="#ch13fn8-marker">8</a></sup>See Sadowski, Caitlin et al. 2018. “Lessons from Building Static Analysis Tools at Google.” <em>Communications of the ACM</em> 61(4): 58–66. doi:10.1145/3188720.</p>
<p data-type="footnote" id="ch13fn9"><sup><a href="#ch13fn9-marker">9</a></sup>See Cousot, Patrick, and Radhia Cousot. 1976. “Static Determination of Dynamic Properties of Programs.” <em>Proceedings of the 2nd International Symposium on Programming</em>: 106–130. <a href="https://www.researchgate.net/publication/239596642_Static_determination_of_dynamic_properties_of_programs"><em class="hyperlink">https://www.researchgate.net/publication/239596642_Static_determination_of_dynamic_properties_of_programs</em></a>.</p>
<p data-type="footnote" id="ch13fn10"><sup><a href="#ch13fn10-marker">10</a></sup>Souyris, Jean et al. 2009. “Formal Verification of Avionics Software Products.” <em>Proceedings of the 2nd World Conference on Formal Methods</em>: 532–546. doi:10.1007/978-3-642-05089-3_34.</p>
<p data-type="footnote" id="ch13fn11"><sup><a href="#ch13fn11-marker">11</a></sup>See, e.g., Kern, Christoph, and Mark R. Greenstreet. 1999. “Formal Verification in Hardware Design: A Survey.” <em>ACM Transactions on Design Automation of Electronic Systems</em> 4(2): 123–193. doi:10.1145/307988.307989. See also Hunt Jr. et al. 2017. “Industrial Hardware and Software Verification with ACL2.” <em>Philosophical Transactions of The Royal Society A Mathematical Physical and Engineering Sciences</em> 375(2104): 20150399. doi: 10.1098/rsta.2015.0399.</p>
<p data-type="footnote" id="ch13fn12"><sup><a href="#ch13fn12-marker">12</a></sup>See Chudnov, Andrey et al. 2018. “Continuous Formal Verification of Amazon s2n.” <em>Proceedings of the 30th International Conference on Computer Aided Verification</em>: 430–446. doi:10.1007/978-3-319-96142-2_26.</p>
</div>
